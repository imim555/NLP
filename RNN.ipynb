{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [기본 조작](#toc1_1_) : RNN, bidrectional, LSTM, GRU\n",
    "- [EX1 : 문자단위 RNN(char RNN)](#toc1_2_)    \n",
    "- [EX2 : 문장단위 RNN](#toc1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[기본 조작](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.8.19\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력차원\n",
    "#---------------\n",
    "# 3D (batch, time, vocab)\n",
    "# 입력데이터는 임베딩층을 통과한 상태라고 하자\n",
    "# 그러므로 input 크기를 dense 크기로 간주한다\n",
    "\n",
    "input_size=5    # dense 크기(임베딩 결과)\n",
    "hidden_size=8   # hidden 크기\n",
    "inputs = torch.Tensor(1,10,5) # 입력텐서(batch,time,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# rnn\n",
    "# -----------------\n",
    "# RNN 셀은 두 개의 입력을 리턴하는데, \n",
    "# 첫번째 리턴값은 모든 시점(timesteps)의 은닉 상태들 \n",
    "# 두번째 리턴값은 마지막 시점(timestep)의 은닉 상태\n",
    "\n",
    "cell = nn.RNN(input_size, hidden_size, batch_first=True, num_layers = 2)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# 양방향RNN(Bidirectional)\n",
    "\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# LSTM(Long short term memory)\n",
    "\n",
    "cell = nn.LSTM(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "outputs, _state = cell(inputs) # _state는 튜플형태태\n",
    "print(outputs.shape) # (batch, time, hidden)\n",
    "print(_state[0].shape) # 마지막 hidden_state\n",
    "print(_state[1].shape) # 마지막 cell_state\n",
    "                 # (num_layers * num_directions, batch_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "cell = nn.GRU(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "outputs, _state = cell(inputs)\n",
    "print(outputs.shape) # (batch, time, hidden)\n",
    "print(_state.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[EX1 : 문자단위 RNN(char RNN)](#toc0_)\n",
    "---\n",
    "- apple 입력 -> pple! 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자집합 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "# 문자집합 생성\n",
    "input = 'apple'\n",
    "label = 'pple!'\n",
    "vocab = sorted(list(set(input+label)))\n",
    "vocab_size = len(vocab)\n",
    "print('문자집합 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "input_size = vocab_size # embedding 단계 이후이므로 input은 dense 의미\n",
    "hidden_size = 10        # 여기서 embedding을 원핫인코딩하면, dense차원은 vocab크기\n",
    "output_size = 5\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성: 정수인코딩 - 정수맵핑 - 텐서변환\n",
    "# ------------------------------------------\n",
    "\n",
    "# 정수인코딩\n",
    "char_to_index = dict((c,i) for i,c in enumerate(vocab))\n",
    "\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "\n",
    "print(char_to_index)\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 정수맵핑\n",
    "x_data = [char_to_index[c] for c in input]\n",
    "y_data = [char_to_index[c] for c in label]\n",
    "\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 원핫인코딩(embedding) 및 3D 텐서변환\n",
    "\n",
    "# 배치차원 추가\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 5]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫벡터 변환\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)\n",
    "\n",
    "# 텐서 변환\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델구현\n",
    "    - $rnn : tanh(XW_x+H_{t-1}W_h + b)=H_t$\n",
    "    - $출력층 : f(H_tWy+b)=Y_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구조\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN층\n",
    "        self.fc = torch.nn.Linear(hidden_size,output_size,bias=True) # 출력층\n",
    "\n",
    "    def forward(self,x):\n",
    "        x, _status = self.rnn(x) # (N,T,H)\n",
    "        x = self.fc(x)  # (N,T,V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 선언\n",
    "net = Net(input_size, hidden_size, output_size) # 신경망\n",
    "criterion = torch.nn.CrossEntropyLoss()         # 손실함수\n",
    "optimizer = optim.Adam(net.parameters(), LR)    # 최적화함수수\n",
    "\n",
    "\n",
    "# output 사이즈 확인\n",
    "outputs = net(X) \n",
    "print(outputs.shape) # 3D (N,T,V)\n",
    "print(outputs.view(-1,vocab_size).shape)  # 2D (NxT,V)\n",
    "print(outputs.data.numpy().argmax(axis=2).shape)  #1D (N,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.5324671268463135 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
      "1 loss:  1.1871622800827026 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
      "2 loss:  0.8183690309524536 prediction:  [[4 4 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppll!\n",
      "3 loss:  0.4565443992614746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "4 loss:  0.21796676516532898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "5 loss:  0.10103382170200348 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.05083095282316208 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.02786228619515896 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.01661837473511696 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.010565690696239471 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.007019279059022665 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.004782765172421932 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.0032958381343632936 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.002289379481226206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.0016153387259691954 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.0011700470931828022 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.0008752761641517282 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.0006763229030184448 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.0005380549700930715 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.00043905858183279634 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.00036615721182897687 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.00031111721182242036 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.000268653966486454 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.0002352671726839617 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.0002086230379063636 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.00018712560995481908 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.00016948852862697095 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0001549017324578017 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.00014272195403464139 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0001324249169556424 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.00012370091280899942 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.00011624009493971244 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.00010985182598233223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.00010432163253426552 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  9.948266961146146e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  9.52395930653438e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  9.156860323855653e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  8.832665480440482e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  8.546609751647338e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  8.291543053928763e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  8.065080328378826e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  7.860071491450071e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  7.678899419261143e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  7.519182690884918e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  7.3761519161053e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  7.245039887493476e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  7.121078670024872e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  7.013805588940158e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  6.91606619511731e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  6.832631333963946e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  6.744427082594484e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  6.672910240013152e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  6.603777728741989e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  6.544181087519974e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  6.49173598503694e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  6.434521492337808e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  6.386844324879348e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  6.343934364849702e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  6.301024404820055e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  6.265265255933627e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  6.227123230928555e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  6.196132744662464e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  6.165142258396372e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  6.136535375844687e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  6.107928493293002e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  6.079320883145556e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  6.055482663214207e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  6.031642988091335e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  6.0101876442786306e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  5.98634869675152e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  5.964892989140935e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  5.94582234043628e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  5.9267513279337436e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  5.912447886657901e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  5.890992542845197e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  5.8766891015693545e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  5.857617361471057e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  5.8456982515053824e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  5.826626511407085e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  5.81470740144141e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  5.7956356613431126e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  5.78133222006727e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  5.769412382505834e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  5.7551089412299916e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  5.743189831264317e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  5.728886026190594e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  5.7145825849147514e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  5.702663474949077e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  5.690743637387641e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  5.678824527421966e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  5.66690468986053e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  5.65021691727452e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  5.6359131122007966e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  5.623994002235122e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  5.614458495983854e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  5.60015432711225e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  5.590618820860982e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  5.573931412072852e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  5.5667798733338714e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  5.552476432058029e-05 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "epochs=100\n",
    "for i in range(epochs):\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), Y.view(-1)) # Batch 차원 제거 => (NxT,vocab), (,NxT)\n",
    "    \n",
    "    optimizer.zero_grad() # 초기화\n",
    "    loss.backward()       # 기울기\n",
    "    optimizer.step()      # 파라미터 업데이트\n",
    "\n",
    "    # 알림\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 단어별 확률분포(vocab size)에서 가장 높은 값의 인덱스 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)]) # 인덱스->char\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1, 0],\n",
      "        [1, 0, 0, 1],\n",
      "        [0, 1, 1, 1]])\n",
      "tensor([[2, 1, 1, 1],\n",
      "        [1, 2, 0, 2]])\n",
      "tensor([[1, 1, 0],\n",
      "        [2, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# argmax(dim=n)\n",
    "# ------------------------\n",
    "# 을 하면 그 차원(n)이 사라지고, 최댓값의 인덱스 배열이 남는다다\n",
    "# 3D 텐서 생성 (크기: 2x3x4)\n",
    "tensor_3d = torch.tensor([\n",
    "    [[1, 5, 2, 3],\n",
    "     [4, 8, 6, 7],\n",
    "     [9, 3, 1, 0]],\n",
    "\n",
    "    [[2, 3, 7, 1],\n",
    "     [5, 1, 4, 8],\n",
    "     [0, 6, 2, 9]]\n",
    "])\n",
    "\n",
    "# dim=0 (첫 번째 차원에서 최댓값의 인덱스)\n",
    "# 첫첫 번째 차원(배치)**을 따라 최댓값의 인덱스를 반환 → 크기 (3, 4)\n",
    "# 배치가 2개이니까 인덱스는 0과1만 할당당\n",
    "print(torch.argmax(tensor_3d, dim=0)) \n",
    "\n",
    "# dim=1 (두 번째 차원에서 최댓값의 인덱스)\n",
    "# 두 번째 차원(행)을 따라 최댓값의 인덱스를 반환 → 크기 (2, 4)\n",
    "# T가 3개이니까 인덱스는 0~2만 할당\n",
    "print(torch.argmax(tensor_3d, dim=1))\n",
    "\n",
    "# dim=2 (세 번째 차원에서 최댓값의 인덱스)\n",
    "# 세 번째 차원(열)을 따라 최댓값의 인덱스를 반환 → 크기 (2, 3)\n",
    "# D가 4개이니까 인덱스는 0~3만 할당당\n",
    "print(torch.argmax(tensor_3d, dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[EX2 : 문장단위 RNN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "# ----------------\n",
    "# embedding : 원핫\n",
    "# data = (batch, time, dense)\n",
    "# rnn output = (batch, time, hidden)\n",
    "# affine output = (batch, time, vocab)\n",
    "input_size = vocab_size \n",
    "hidden_size = 25\n",
    "time_size = 10\n",
    "LR = 0.1\n",
    "EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터구성 : 정수인코딩 - 정수맵핑 - 텐서변환\n",
    "# -------------------------------------------\n",
    "\n",
    "# 정수인코딩\n",
    "vocab = list(set(sentence))\n",
    "char_dict = {c:i for i, c in enumerate(vocab)}\n",
    "vocab_size = len(vocab) # 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you wan ---> f you want\n",
      "f you want --->  you want \n",
      " you want  ---> you want t\n",
      "you want t ---> ou want to\n",
      "ou want to ---> u want to \n",
      "u want to  --->  want to b\n",
      " want to b ---> want to bu\n",
      "want to bu ---> ant to bui\n",
      "ant to bui ---> nt to buil\n",
      "nt to buil ---> t to build\n",
      "t to build --->  to build \n",
      " to build  ---> to build a\n",
      "to build a ---> o build a \n",
      "o build a  --->  build a s\n",
      " build a s ---> build a sh\n",
      "build a sh ---> uild a shi\n",
      "uild a shi ---> ild a ship\n",
      "ild a ship ---> ld a ship,\n",
      "ld a ship, ---> d a ship, \n",
      "d a ship,  --->  a ship, d\n",
      " a ship, d ---> a ship, do\n",
      "a ship, do --->  ship, don\n",
      " ship, don ---> ship, don'\n",
      "ship, don' ---> hip, don't\n",
      "hip, don't ---> ip, don't \n",
      "ip, don't  ---> p, don't d\n",
      "p, don't d ---> , don't dr\n",
      ", don't dr --->  don't dru\n",
      " don't dru ---> don't drum\n",
      "don't drum ---> on't drum \n",
      "on't drum  ---> n't drum u\n",
      "n't drum u ---> 't drum up\n",
      "'t drum up ---> t drum up \n",
      "t drum up  --->  drum up p\n",
      " drum up p ---> drum up pe\n",
      "drum up pe ---> rum up peo\n",
      "rum up peo ---> um up peop\n",
      "um up peop ---> m up peopl\n",
      "m up peopl --->  up people\n",
      " up people ---> up people \n",
      "up people  ---> p people t\n",
      "p people t --->  people to\n",
      " people to ---> people tog\n",
      "people tog ---> eople toge\n",
      "eople toge ---> ople toget\n",
      "ople toget ---> ple togeth\n",
      "ple togeth ---> le togethe\n",
      "le togethe ---> e together\n",
      "e together --->  together \n",
      " together  ---> together t\n",
      "together t ---> ogether to\n",
      "ogether to ---> gether to \n",
      "gether to  ---> ether to c\n",
      "ether to c ---> ther to co\n",
      "ther to co ---> her to col\n",
      "her to col ---> er to coll\n",
      "er to coll ---> r to colle\n",
      "r to colle --->  to collec\n",
      " to collec ---> to collect\n",
      "to collect ---> o collect \n",
      "o collect  --->  collect w\n",
      " collect w ---> collect wo\n",
      "collect wo ---> ollect woo\n",
      "ollect woo ---> llect wood\n",
      "llect wood ---> lect wood \n",
      "lect wood  ---> ect wood a\n",
      "ect wood a ---> ct wood an\n",
      "ct wood an ---> t wood and\n",
      "t wood and --->  wood and \n",
      " wood and  ---> wood and d\n",
      "wood and d ---> ood and do\n",
      "ood and do ---> od and don\n",
      "od and don ---> d and don'\n",
      "d and don' --->  and don't\n",
      " and don't ---> and don't \n",
      "and don't  ---> nd don't a\n",
      "nd don't a ---> d don't as\n",
      "d don't as --->  don't ass\n",
      " don't ass ---> don't assi\n",
      "don't assi ---> on't assig\n",
      "on't assig ---> n't assign\n",
      "n't assign ---> 't assign \n",
      "'t assign  ---> t assign t\n",
      "t assign t --->  assign th\n",
      " assign th ---> assign the\n",
      "assign the ---> ssign them\n",
      "ssign them ---> sign them \n",
      "sign them  ---> ign them t\n",
      "ign them t ---> gn them ta\n",
      "gn them ta ---> n them tas\n",
      "n them tas --->  them task\n",
      " them task ---> them tasks\n",
      "them tasks ---> hem tasks \n",
      "hem tasks  ---> em tasks a\n",
      "em tasks a ---> m tasks an\n",
      "m tasks an --->  tasks and\n",
      " tasks and ---> tasks and \n",
      "tasks and  ---> asks and w\n",
      "asks and w ---> sks and wo\n",
      "sks and wo ---> ks and wor\n",
      "ks and wor ---> s and work\n",
      "s and work --->  and work,\n",
      " and work, ---> and work, \n",
      "and work,  ---> nd work, b\n",
      "nd work, b ---> d work, bu\n",
      "d work, bu --->  work, but\n",
      " work, but ---> work, but \n",
      "work, but  ---> ork, but r\n",
      "ork, but r ---> rk, but ra\n",
      "rk, but ra ---> k, but rat\n",
      "k, but rat ---> , but rath\n",
      ", but rath --->  but rathe\n",
      " but rathe ---> but rather\n",
      "but rather ---> ut rather \n",
      "ut rather  ---> t rather t\n",
      "t rather t --->  rather te\n",
      " rather te ---> rather tea\n",
      "rather tea ---> ather teac\n",
      "ather teac ---> ther teach\n",
      "ther teach ---> her teach \n",
      "her teach  ---> er teach t\n",
      "er teach t ---> r teach th\n",
      "r teach th --->  teach the\n",
      " teach the ---> teach them\n",
      "teach them ---> each them \n",
      "each them  ---> ach them t\n",
      "ach them t ---> ch them to\n",
      "ch them to ---> h them to \n",
      "h them to  --->  them to l\n",
      " them to l ---> them to lo\n",
      "them to lo ---> hem to lon\n",
      "hem to lon ---> em to long\n",
      "em to long ---> m to long \n",
      "m to long  --->  to long f\n",
      " to long f ---> to long fo\n",
      "to long fo ---> o long for\n",
      "o long for --->  long for \n",
      " long for  ---> long for t\n",
      "long for t ---> ong for th\n",
      "ong for th ---> ng for the\n",
      "ng for the ---> g for the \n",
      "g for the  --->  for the e\n",
      " for the e ---> for the en\n",
      "for the en ---> or the end\n",
      "or the end ---> r the endl\n",
      "r the endl --->  the endle\n",
      " the endle ---> the endles\n",
      "the endles ---> he endless\n",
      "he endless ---> e endless \n",
      "e endless  --->  endless i\n",
      " endless i ---> endless im\n",
      "endless im ---> ndless imm\n",
      "ndless imm ---> dless imme\n",
      "dless imme ---> less immen\n",
      "less immen ---> ess immens\n",
      "ess immens ---> ss immensi\n",
      "ss immensi ---> s immensit\n",
      "s immensit --->  immensity\n",
      " immensity ---> immensity \n",
      "immensity  ---> mmensity o\n",
      "mmensity o ---> mensity of\n",
      "mensity of ---> ensity of \n",
      "ensity of  ---> nsity of t\n",
      "nsity of t ---> sity of th\n",
      "sity of th ---> ity of the\n",
      "ity of the ---> ty of the \n",
      "ty of the  ---> y of the s\n",
      "y of the s --->  of the se\n",
      " of the se ---> of the sea\n",
      "of the sea ---> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 정수맵핑\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - time_size):\n",
    "    x_str = sentence[i:i+time_size]\n",
    "    y_str = sentence[i+1:i+1+time_size]\n",
    "    print(x_str,'--->', y_str)\n",
    "\n",
    "    x_data.append([char_dict[c] for c in x_str])\n",
    "    y_data.append([char_dict[c] for c in y_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 17, 12, 8, 6, 17, 4, 24, 1]\n",
      "[2, 17, 12, 8, 6, 17, 4, 24, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])  # if you wan에 해당됨.\n",
    "print(y_data[0])  # f you want에 해당됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩(embedding) 및 텐서변환\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "X = torch.FloatTensor(x_one_hot)  # (170,10,25)\n",
    "Y = torch.LongTensor(y_data)  # (170,10,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구조\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, vocab_size, bias=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x, _status = self.rnn(x) # x: output(모든 타임스텝의 은닉상태), _status: 마지막 타임스텝의 은닉상태\n",
    "        x = self.fc(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선언언\n",
    "net = Net(input_size, hidden_size, 2)   # 신경망\n",
    "criterion = torch.nn.CrossEntropyLoss() # 손실함수\n",
    "optimizer = optim.Adam(net.parameters(), LR)  # 최적화함수수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n",
      "torch.Size([1700, 25])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "source": [
    "# 결과과 차원확인\n",
    "outputs = net(X)\n",
    "print(outputs.shape)  # 3D (NL170,T:10,V:25)\n",
    "print(outputs.view(-1,vocab_size).shape) # 2D (NxT:1700,V:25)\n",
    "print(Y.view(-1).shape) # 1D (,NxT:1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbbbbbbbcbbbbbbbbcbbbbbbcbbbbbbbbbbbbbbbbbbbcbbbbbbb.bbcbbbbcbbcbbbbbbbcbbbbbbbbbbbbbbbbcbbbb.bbbbbbbbbbbbbbcbbbbbbbbbbb.bbbbcbb.bb.bbbbcbbbbcbbcbbb.bbcbbbbbbbcbbbbbcbbbbbbb.bbbcb\n",
      "                        p                              p                                 p                              p       p                                                  \n",
      "                                                                                                                                                                                   \n",
      "eddedddeddededdodddddododdd dodod doddededdedddddde eeddeeddddeddeededdddededddodddsdodtsddde dededdeodddododdd dedoddddddedddddddeedededdeddedddeddddd ded dedodeddddd deddededdde\n",
      " rr rr  rr ur rr rr ur rrhr rr r  rr rr rrrr rr rr rrr rr rr    r  rr rr r  rrrrrr rrr urerr rr  r  r rh rrhrr rrr rr rr rr  rr  r  rr rrrr rr rrrr rr rr  rr  rrrr rr rr  r  r  u \n",
      "           t  tlt   t t w w t     t  t t     w      w t   t  w t     t    t   w t t t  whtt     t  mh t tltht   t t   t   lt  l t     t  wht  t  k.           w tl t    t  t       \n",
      "  th  t      tt     t t t e t t t t a    t     t     w t t  tthtttt  t tt t t t t t t t t t    tt  tt t t t t t t t t t t h   whtt t  t  t tt t t t t t t t t t t  ttt  t tt   t   \n",
      "  t                 t       t t     t  t  t    t                     t        t   t t     t     t     t t       t   t t               t  t  t t       t t t   t     t      t       \n",
      "  t et e   th t t   ththe  et e   the  d    e    t e    e t et e  e  e  e t t t e  ethe   e  e  e e   the t e  eththe e e  e e  ethe  e e  e  d   t e     e   e e    e e  et e t e \n",
      "t thd thdot hd  ht ethththt ththt dothe   e edoth   hththth edt dee  thh  te othd thtothdhtht etthdoththththt thtothtothdhtht ttoththttt t hdhthe thd thdothd the thtdthhtdtht thd \n",
      "t t   t      ete  d t   d   d d t d w  ts  eeeee e d d   dt e e dee  t   et   i   t t t d d e eet d t   t   e t d t d t d d  dst d   dthete d t e t   t e w   t e t   et eet e t   \n",
      "t t  tt dt   dt   t d   t  dt   t d  t d   s  e  d   ee d t  t   e d t  dlt   t  tt d   t  t  e t d   d   t   e   t d      t  t  t  t t  t    t  tt  tt d t d t  te  te    t   t  t\n",
      "t to t  dto   d   e d t  t  do t  d  todt  l  l  to   o   to to t  d dt  lto lto t  d  dt  to t t  t  t   to t  d t t      to t  t  t to t  t     t  t  dte d t  te  t  d  to  t  t\n",
      "t to  t  to   t t d tod e   to  tot  tot  t  r   to       to t     t t  r to  to    t ht   to   to  e to  to  e tot t t  l to  o to   to t  t t r to  tod eod to  ee d     to  to  \n",
      "t to  ee   to  et r tot e e t   tot e  t  te r e to  ee   to     e t r  rewh  to  eht   rh to   tot   to  to  e tot r t  leto    to   to  e t to  to  tot e t to  eeot it  to  to  \n",
      "t to  eht  to  et r tht   e to  tot etot et  r e to  eee  th  h  e t t  r tht to  eht ht h th   tht e tht to  e tht d th   th    th   th  h t to  th  tht eot eo  eeht     th  th  \n",
      "t toeptht hto t t e tht e e to ptot et t  pe r e toe ee   theth  e t t    tht eoaphot h ph the  tht e the toe e tht t the  thephethe  th th t toe the tot e t toe eeot  t  the the \n",
      "t toeptoepht  pei   tot e e toemtot e  to pe   e toe eh p toeth pe t t    tot eoeptot hiph toep toe e t   toe e thi t toe  toephethep th th t toa toeptot e t eoe eeht  t  toeptoep\n",
      "t toeptoepot  pei   tot e e t emtot e  t  pe  pe toe ee p toeph pe t t    t s eoeptotch p  toep toe e t   toe t toi t toe  toeph toep to ph t tod toeptos e t to  eeot  t  toept ep\n",
      "t toeptospos  pei , tot e e toeptod em t dpe   e toe ehe  toepo pe t t    t s t dptod h t  toe  tos s tod toe e toi t toe  toepoetoe  toepo t tod toe t t e t to  ee t  t  toe t ep\n",
      "t toeltodloso pei , tod ele toeptod et todpe l e toel,he  toeco le tod  d t d t dptod a p  toe  tod s tod toe e toi d toel toepoetoe  toeco p tod t e t d e t t   e, t  d  toe t dl\n",
      "m to lthsloto cei , thd ele t  ltod  t t  le l e toel,he  to coele tod  d t i t  ltodlail  toel thd s tod to l, t i d thel t  lh toel theto l tod t elt d e tod   e,st  d  t elt el\n",
      "m to lthelotoncei , thdoele to ltod em t  te dee toel,he  to conle d to r t d t nltod sil  the  thd i t d to l, thi d thel toelh thel theto l tod toe t d e i d   e,ii  d  toelthel\n",
      "m tonlton'hdoetei , thdoe', ton'tod em d  te dee toel,he  theth le t to r t d t n'tod sig  the  the e t d to  , thi d thel toe h the  theto g tod toe tod e i t  e tit  d  the thel\n",
      "t ton'thnthdontui , t doe', t n'tod em r  te ree tonethe  to th le t do r t d t n'tod sig  the  thd s t d to  t thi r thel toeth thel thnto g tod toelt d e i t  ectit  d  thelth l\n",
      "t to pth thdon'ui d a doe'e d n'tod em r  'e rle tonethe  to thnle t to r and t n'tod sig  ther tha s t d to  t thi r ther toeth ther thnte ' tod tnert d e sit  eptit  d  therthir\n",
      "p to rth thdon'ui d andoe'e d nptod um r  pe rle theether to ponle t to r and ann'tod sig  ther tha s tnd ao  t thi r ther torth ther thnto p tod tnertnt e s t  eptit  r  therthip\n",
      "t to rth phdo cui d a soep, d nptod um r  pe rle theether to ponle t to r and annptodtsig  ther tha s tod ao  , thi r ther torthether thnto p tod thertnd e s t  etsito r  therthdp\n",
      "t to lth chto cui d tnsoep, donptod um r  te rle theether to ponle t to l and annptodtsig  thel tha s tns ao  , tht r ther torth thel th co g tod thelsnd e s t  ecsito r  thelshdp\n",
      "g to ltonthto cui d ansoep, donptod um rm te rle theethel to conle t to l ans annctodtsig  thel thd s tns ao  , tut r thel to th thel th co g tod thelsnd e s t  emsito d  thelseip\n",
      "g to ltonthto cuimd ansou', donptod um rt ce  le thelthel to conle t to l ans ann'todtsig  thel thsks tns ao  , tui   thel to th thel th co g tod thelsnd e s tm emsity d  thelseip\n",
      "g to ltonchto cui d andoi', don'tod um dm le rle theethel te 'onle t to l and a e'todtsig  thel thsks tnd ao k, tui r thel teacs thel th cong fod thelsnd e s dm emsity d  thelsedt\n",
      "' to ltanchto cui d ansoi', don'tod um dm de lle theethel te 'onle t do l and ann'todtsign thel tosks tnd ao k, tui r thel teacsnthel to 'ongnf dktaelsnd e s dm ecsit  r  thelsea'\n",
      "' to rsonchto cuild a dhigs don'tod im dm le rle theether to conee t to l aod d e'todssig rthem tosks tod ao k, tui r ther toach the' to cong f d thelsnd ess ds e'sito r  themseat\n",
      "p to rsont to cuild ans ig, dongt d um rm re rle theether te 'onee t donr and anngt dtsign ther tosks tnd aonk, tui r ther teach ther to pongnf r thersnd e s ts etsity rf therseac\n",
      "p to rsonc to cuild anshig, don't d um dp pe rle theether te conle t tonr and aon't dtsign ther tosks tnd ao k, tui r ther teach ther to cong f r thersnd ess ts ecsity rf therteac\n",
      "p to rsonc to cuild a ship, don't d um rp pecrle theether th conle t to r and aon't dssig  ther tosks tnd to k, tut r the  toach ther to con' f d thersnd ess ts etsity rf therteat\n",
      "p to psonc to cuild a shig, don't d um dp pe rle t  ether to conle t do r and aon't dssign ther tosks and ao k, tut r ther teach ther to pong f r thersnd ess ts etsity df therted'\n",
      "p to lsonc to build anshig, don't dfum dp pe rle t gethe  to conle t donl and aon't dssign ther tosks and aonk, but r ther teach the  to congnf r thersnd ess tp etsity df therteac\n",
      "p to lsont to cuild a shi', don't d um rpspe rle t eethe  to conlect do l and ton't dssign the  tosks and tonk, but r the  toach the  to congnf r the wnd ess ts etsity df the teac\n",
      "p to lsont to cuild a shi', don't d um up pe rle th ether te conlect do l and ton't dssign ther tosks and to k, but r ther teach ther to cong f r thersnd ess ts etsity df therseac\n",
      "p to rwont to build a ship, don't d um up pe rle to ether te conle t do r and aon't dssign ther tosks and to k, but r ther teach ther to cong f r thersnd em  tp ensity df therseac\n",
      "p bo rwont to build a ship, don't d um up ge rle to ethe  te conle t do r and aon't dssign ther tosks and ao k, but r ther teach the  to cong f r the sndmess ts ensity rf the seac\n",
      "p bo rsant to build a ship, don't d um up pe rle to ethe  to conlect do r and aon't dssign the  tosks and do k, but r ther toach the  to cong f r the snd ess ts ensity rf the seac\n",
      "p to rsant to cuild a ship, don't d um up pe rle to ethe  te collect do r and don't dssign the  tosks and do k, but r ther teach the  to cong f r the snd ess ts ensity rf the seac\n",
      "p to rwant to build a ship, don't drum up pe ple to ethe  te conlect do r and don't dssign ther tosks and dork, but r ther teach the  to cong for therend ess ds ensity rf the seac\n",
      "p bo rwant to build a ship, don't drum up pe ple to ethe  te conlect dorr and don't dssign the  tosks and dork, but r ther teach the  to cong for the end ess ts ensity rf the eeac\n",
      "p bo rwont to build a ship, don't drum up people togethe  te conlect dorr and don't dssign the  tosks and dork, but r ther teach them to cong for the end ess ts ensity rf themseac\n",
      "p bo rwant to build a ship, don't drum up people to ethem te bonlect dorr and don't dssign the  tosks and dork, but r the  teach them to cong for the endless tm ensity of themseac\n",
      "p bo rwant to build a ship, don't drum up people to ethem te bollect worl and don't dssign them tosks and dork, but r the  teach them to cong for themenwless tm ensity of themseac\n",
      "p bo rwant to build a ship, don't drum up people to ethem to collect dorl and don't dssign them tosks and dork, but r the  teach them to cong for themenwless tm ensity of themseac\n",
      "p bogrwant to build a ship, don't drum up people togethem to collect dorl and don't dssign them tosks and dork, but r the  teach them to cong for themendless tm ensity of themeeac\n",
      "p bourwant to build a ship, don't drum up people togethem te collect worr and won't dssign them tosks and dork, but r the  teach them to cong for the endless tm ensity of themeeac\n",
      "p tourwant to build a ship, don't drum up people to ethem to collect worr and won't dssign them tosks and work, but r the  toach them to cong for themenwless tmmensity of themseac\n",
      "p bourwant to build a ship, don't drum up people to ethe  to collect worr and won't dssign them tosks and work, but r the  toach them to long for themenwless tmmensity of themseac\n",
      "p bourwant to build a ship, don't drum up people togethe  te collect worr and won't dssign them tosks and work, but r the  teach them to long for the enwless wmmensity of themseac\n",
      "p bourwant to cuild a ship, don't drum up people togethe  to collect worr and won't dssign them tosks and work, but r the  toach them to long for the endless tmmensity of themseac\n",
      "p tourwant to build a ship, don't drum up people togethe  te collect worr and won't dssign the  tosks and work, but r the  teach them to long for the endless immensity of themseal\n",
      "p tourwant to build a ship, don't drum up people togethe  to collect worr and won't dssign them tosks and work, but r the  toach them to long for the endless immensity of themseac\n",
      "p tourwant to build a ship, don't drum up people togethe  to collect worr and won't dssign them tosks and work, but r the  toach them ta long for the endless immensity of themseac\n",
      "t tourwant to build a ship, don't drum up people togethe  to collect worr and don't dssign them tasks and work, but r the  toach them ta long for the endless immensity of themseac\n",
      "t tourwant to build a ship, don't drum up people togethe  to collect worr and don't assign them tasks and work, but r the  toach them ta long for the endless immensity of themseac\n",
      "t tourwant to build a ship, don't drum up people togethe  te bollect worr and don't assign them tasks and work, but r the  teach them ta long for the endless immensity of themseac\n",
      "t tourwant to build a ship, don't arum up people togethe  to collect worr and won't assign them tasks and work, but r the  toach them ta nong for the endless immensity of themseac\n",
      "g tourwant to build a ship, don't drum up people togethe  to collect worr and won't assign them tosks and work, but r the  toach them to long for the endless immensity of themseac\n",
      "t tourwant to build a ship, don't arum up people togethe  te collect word and don't assign them tosks and work, but r the  toach them to long for the endless immensity of the seac\n",
      "t toudwant to build a ship, don't arum up people togethe  te bollect wood and don't assign them tosks and work, but r ther teach them to long for the endless immensity of themseac\n",
      "g toudwant to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but r ther toach them to long for the endless immensity of the seac\n",
      "g toudwant to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but r ther toach them to long for the endless immensity of the seac\n",
      "t toudwant to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rmther toach them ta long for the endless immensity of themseac\n",
      "f boudwant to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but r ther toach them ta long for the endless immensity of the seac\n",
      "t boudwant to build a ship, don't arum up people together te collect wood and don't assign the  tasks and work, but rather teach them ta long for the endless immensity of themseac\n",
      "t boudwant to build a ship, don't arum up people together to collect wood and won't assign them tosks and work, but r ther toach them to long for the endless immensity of the seac\n",
      "g boudwant to build a ship, don't drum up people together to collect woor and don't assign them tosks and work, but r ther toach them to long for the endless immensity of themseac\n",
      "t boudwant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of themseac\n",
      "t boudwant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of themseac\n",
      "t boudwant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of themseac\n",
      "t boudwant to build a ship, don't drum up people together te bollect woor and don't assign them tosks and work, but rather teach them to long for the endless immensity of themseac\n",
      "t bou'want to build a ship, don't drum up people together te collect woor and don't assign them tosks and work, but rather toach them to long for the endless immensity of themseac\n",
      "g boudwant to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of themseat\n",
      "g boudwant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the seat\n",
      "p boudwant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the seac\n",
      "t boudwant to build a ship, don't drum up people together to collect wood and don't acsign them tasks and work, but rather toach them ta long for the endless immensity of the seac\n",
      "g doudwant to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the seac\n",
      "t doudwant to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "f toudwant to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "f boudwant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "g toudwant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless gmmensity of the seac\n",
      "g toudwant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t toudwant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the seac\n",
      "p toudwant to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
      "p toudwant to build a ship, don't arum up people together to collect wood and don't assign the  tasks and work, but rather toach the  ta long for the endless immensity of the seac\n",
      "t toudwant to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of themseac\n",
      "t toudwant to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of themseac\n",
      "p boudwant to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of themseac\n",
      "p boudwant to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of themsea.\n",
      "p bou'want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of themseac\n",
      "p boudwant to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of themseac\n",
      "t youdwant to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of themsea.\n",
      "t youdwant to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "epochs=100\n",
    "for i in range(epochs):\n",
    "    outputs = net(X)  # 입력(170,10,25)=>출력(170,10,25)\n",
    "    loss = criterion(outputs.view(-1,vocab_size),Y.view(-1))\n",
    "    \n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 알림\n",
    "    results = outputs.argmax(axis=2)\n",
    "    predict_str = ''\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0:  # 처음에는 예측 결과를 모두 가져오지만\n",
    "            predict_str += ''.join([vocab[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += vocab[result[-1]]\n",
    "\n",
    "    print(predict_str)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH38_CPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
