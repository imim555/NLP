{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [seq2se2 실습](#toc1_1_)    \n",
    "  - [데이터 로딩](#toc1_2_)    \n",
    "  - [전처리](#toc1_3_)    \n",
    "  - [데이터 변환](#toc1_4_)    \n",
    "  - [데이터 분리 : train & test](#toc1_5_)    \n",
    "  - [데이터로더](#toc1_6_)    \n",
    "  - [모델링](#toc1_7_)    \n",
    "  - [모델 로딩](#toc1_8_)    \n",
    "  - [동작시키기 (test 모드)](#toc1_9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[seq2se2 실습](#toc0_)\n",
    "- 영어->프랑스어 변역기\n",
    "- python -v 3.8.19\n",
    "- pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[데이터 로딩](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "# 파일 다운로드\n",
    "import requests\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "filename = \"fra-eng.zip\"\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "with open(filename, \"wb\") as file:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        file.write(chunk)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip complete!\n"
     ]
    }
   ],
   "source": [
    "# 압축 풀기\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"fra-eng.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"destination_folder\")  # 원하는 경로로 변경 가능\n",
    "\n",
    "print(\"Unzip complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[전처리](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    # 프랑스어 악센트 삭제\n",
    "    # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 악센트 삭제 함수 호출\n",
    "    sent=unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백 생성\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "def load_preprocessed_data():\n",
    "  encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "  with open(\"fra.txt\", \"r\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "      # source 데이터와 target 데이터 분리\n",
    "      src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "      # source 데이터 전처리\n",
    "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "      \n",
    "      # target 데이터 전처리\n",
    "      tar_line = preprocess_sentence(tar_line)\n",
    "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "      encoder_input.append(src_line)\n",
    "      decoder_input.append(tar_line_in)\n",
    "      decoder_target.append(tar_line_out)\n",
    "\n",
    "      if i == num_samples - 1:\n",
    "        break\n",
    "\n",
    "  return encoder_input, decoder_input, decoder_target\n",
    "# 디코더 입력 데이터를 생성한 이유\n",
    "# 모델구조는 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다\n",
    "# 하지만, 학습과정에서는 학습 효율을 높이기 위해 이전 디코더 셀의 예측값 대신 실제값을 넣어주므로\n",
    "# 별로도 디코터 입력 데이터도 만든 것이다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "# ['go', '.'] => 문장1개로 간주하자\n",
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[데이터 변환](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어집합 생성\n",
    "# -----------------\n",
    "# word <-> index\n",
    "# 빈도순으로 정수 정렬\n",
    "# 0번<PAD>, 1번<UNK>, 2번<빈도수 가장 많은 단어> ...\n",
    "\n",
    "def build_vocab(sents):\n",
    "    word_list = []\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word_list.append(word)\n",
    "    # 빈도수 계산 및 정렬\n",
    "    word_counts = Counter(word_list)\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    word_to_index={}\n",
    "    word_to_index['<PAD>']=0\n",
    "    word_to_index['<UNK>']=1\n",
    "    for index, word in enumerate(vocab):\n",
    "        word_to_index[word] = index+2\n",
    "\n",
    "    return word_to_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 292, 프랑스어 단어 집합의 크기 : 635\n",
      "{0: '<PAD>', 1: '<UNK>', 2: '.', 3: '!', 4: 'i', 5: 'it', 6: 'get', 7: 'go', 8: 'm', 9: 'away', 10: 'out', 11: 'me', 12: '?', 13: 'up', 14: 'tom', 15: 'be', 16: 'lost', 17: 'on', 18: 'come', 19: 'ahead', 20: 'we', 21: 'run', 22: 'down', 23: 's', 24: 'beat', 25: 'won', 26: 'terrific', 27: 'you', 28: 'back', 29: 'this', 30: 'try', 31: 'no', 32: 'who', 33: 'relax', 34: 'way', 35: 'fair', 36: 'nice', 37: 'us', 38: 'how', 39: 'look', 40: 'help', 41: 'hold', 42: 'calm', 43: 'off', 44: 'forget', 45: 'see', 46: 'got', 47: 'him', 48: 'he', 49: 'shut', 50: 'fun', 51: 'leave', 52: 'don', 53: 't', 54: 'stop', 55: 'wait', 56: 'call', 57: 'take', 58: 'did', 59: 'use', 60: 'sit', 61: 'excuse', 62: 'hello', 63: 'now', 64: 'in', 65: 'left', 66: 'paid', 67: 'home', 68: 'hang', 69: 'll', 70: 'kill', 71: 'open', 72: 'am', 73: 'brief', 74: 'good', 75: 'hurry', 76: 'ok', 77: 'really', 78: 'ask', 79: 'cool', 80: 'goodbye', 81: 'sad', 82: 'drive', 83: 'push', 84: 'wake', 85: 'grab', 86: 'let', 87: 'what', 88: 'trust', 89: 'catch', 90: 'over', 91: 'attack', 92: 'buy', 93: 'cheers', 94: 'thanks', 95: 'awesome', 96: 'burn', 97: 'bury', 98: 'drop', 99: 'fold', 100: 'hire', 101: 'fat', 102: 'hit', 103: 'keep', 104: 'lie', 105: 'lock', 106: 'tell', 107: 'win', 108: 'can', 109: 'job', 110: 'have', 111: 'cute', 112: 'lazy', 113: 'sure', 114: 'she', 115: 'here', 116: 'taste', 117: 'they', 118: 'touch', 119: 'watch', 120: 'after', 121: 'feel', 122: 'hi', 123: 'wow', 124: 'duck', 125: 'smile', 126: 'hug', 127: 'fell', 128: 'know', 129: 'quit', 130: 'tried', 131: 'mad', 132: 'wet', 133: 'join', 134: 'kiss', 135: 'too', 136: 'perfect', 137: 'aim', 138: 'still', 139: 'spoke', 140: 'goofed', 141: 'fine', 142: 'free', 143: 'late', 144: 'ugly', 145: 'may', 146: 'came', 147: 'died', 148: 'speak', 149: 'some', 150: 'honest', 151: 'seated', 152: 'fantastic', 153: 'follow', 154: 'fire', 155: 'hide', 156: 'jump', 157: 'begin', 158: 'eat', 159: 'hop', 160: 'lied', 161: 'slow', 162: 'runs', 163: 'agree', 164: 'dozed', 165: 'froze', 166: 'stood', 167: 'swore', 168: 'kick', 169: 'low', 170: 'move', 171: 'pull', 172: 'show', 173: 'sign', 174: 'skip', 175: 'wash', 176: 'welcome', 177: 'high', 178: 'them', 179: 'a', 180: 'man', 181: 'cheer', 182: 'find', 183: 'fix', 184: 'real', 185: 'phoned', 186: 'refuse', 187: 'rested', 188: 'saw', 189: 'stayed', 190: 'pay', 191: 'busy', 192: 'deaf', 193: 'full', 194: 'game', 195: 'okay', 196: 'tidy', 197: 'well', 198: 've', 199: 'works', 200: 'his', 201: 'new', 202: 'marry', 203: 'prove', 204: 'save', 205: 'speed', 206: 'warn', 207: 'for', 208: 'write', 209: 'chill', 210: 'soon', 211: 'dogs', 212: 'bark', 213: 'die', 214: 'film', 215: 'oh', 216: 'sorry', 217: 'exhale', 218: 'fled', 219: 'hunt', 220: 'knit', 221: 'pass', 222: 'swim', 223: 'inhale', 224: 'listen', 225: 'kind', 226: 'cried', 227: 'drove', 228: 'fired', 229: 'smoke', 230: 'snore', 231: 'stink', 232: 'waved', 233: 'fit', 234: 'ill', 235: 'shy', 236: 'pair', 237: 'so', 238: 'long', 239: 'care', 240: 'ran', 241: 'brave', 242: 'buzz', 243: 'cover', 244: 'cuff', 245: 'tries', 246: 'guys', 247: 'deep', 248: 'rude', 249: 'wise', 250: 'cursed', 251: 'failed', 252: 'forgot', 253: 'helped', 254: 'jumped', 255: 'looked', 256: 'moaned', 257: 'nodded', 258: 'obeyed', 259: 'sighed', 260: 'smiled', 261: 'talked', 262: 'bald', 263: 'cold', 264: 'done', 265: 'fast', 266: 'glad', 267: 'rich', 268: 'safe', 269: 'sick', 270: 'tall', 271: 'thin', 272: 'weak', 273: 'helps', 274: 'hurts', 275: 'hot', 276: 'odd', 277: 'red', 278: 'say', 279: 'stand', 280: 'knew', 281: 'lies', 282: 'went', 283: 're', 284: 'answer', 285: 'strong', 286: 'birds', 287: 'fly', 288: 'bless', 289: 'choose', 290: 'do', 291: 'cry'}\n"
     ]
    }
   ],
   "source": [
    "# word -> index \n",
    "src_to_index = build_vocab(sents_en_in)\n",
    "tar_to_index = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_to_index)\n",
    "tar_vocab_size = len(tar_to_index)\n",
    "\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n",
    "\n",
    "\n",
    "# index -> word\n",
    "index_to_src = {idx:word for word,idx in src_to_index.items() }\n",
    "index_to_tar = {idx:word for word,idx in tar_to_index.items() }\n",
    "print(index_to_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수인코딩\n",
    "\n",
    "def texts_to_sequences(sents, word_to_index):\n",
    "    encoded_X_data = []\n",
    "    for sent in tqdm(sents):\n",
    "        index_sequences = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                index_sequences.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                index_sequences.append(word_to_index['<UNK>'])\n",
    "        encoded_X_data.append(index_sequences)\n",
    "    return encoded_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 997693.63it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 672379.61it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 992969.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [122, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input = texts_to_sequences(sents_en_in, src_to_index)\n",
    "decoder_input = texts_to_sequences(sents_fra_in, tar_to_index)\n",
    "decoder_target = texts_to_sequences(sents_fra_out, tar_to_index)\n",
    "\n",
    "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
    "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
    "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
    "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "# 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
    "def pad_sequences(sentences, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence)!=0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (1000, 4)\n",
      "디코더의 입력의 크기(shape) : (1000, 10)\n",
      "디코더의 레이블의 크기(shape) : (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input)\n",
    "decoder_input = pad_sequences(decoder_input)\n",
    "decoder_target = pad_sequences(decoder_target)\n",
    "\n",
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[데이터 분리 : train & test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [ 41 146 751 878 207 200 267 226  18 491 532 704 240 431 735 669 797 114\n",
      " 199 411 992 148 332 487 934 790 501 334 879  12 765 281 776 230 128 274\n",
      " 864 695 579 123 989 338  78 591 632 127 508 941 988 781 667 185 285 227\n",
      " 628 575 357 914 192 197 867 356 510 570 284 147  84 153 690  49 416 264\n",
      " 601 745 473 824 142 345 590 196 438 149 268 234 688 377 379 829 425 724\n",
      " 201 771  16 571 373 925 412 204 576 372 808 811 728 328 513  24 581 855\n",
      " 793 287 497 454 674 239 946  72 595 869 395 315 235 538 602 344 172 291\n",
      " 320 557 477 218 979 392 792 701 694 206 202 882 703 653 654 672 457 109\n",
      " 709 289 748 488 164 173 364 780  20 549 523 664 608 647 852 189 767 452\n",
      " 191 959 723 763 479 730 537 390 298 593 933 801 536 263 913 594 546 613\n",
      " 978 599 820 794 569 167 693 208 405 783 525 901 339 717 312  67 307 245\n",
      " 905 280 451 875 432 727 407 651 131 539 880 439 916  42 352 301 822 308\n",
      " 708 237 460 220 455 231 677 706 182 100 834 626 659 258 909  83 838 154\n",
      " 676 691 175 721  77 851 892  65 367 388 326  43 136 935 953  47 718 409\n",
      " 296 310 784 255  38 251 524 631 266 668 896 515 876 531 722 835  62 858\n",
      "  75 888 993  22 924  35 943 687 692 582 812 874 369 954 877 951 354 936\n",
      " 238 427 262 895 348 915 423 680 387 516 884  21 459 429 682 923 833 282\n",
      " 814 857 656 297 286 614 121 619 111 355   1 747 130  39 827  44 995 340\n",
      " 254  53 719 973  73 563 110 481 443 658 981  93 750 318 382 902 221 505\n",
      "  91 873   6 819 273 743 475 329 243 324 906 399  80 396 216 823 535 422\n",
      " 360 966 705 257  14 224 135 490 269 378 453 215 108 363 862 317 433 319\n",
      " 279 351  92  32 177  15 113 184 663 861 768 762 498  33 313 158 802 543\n",
      " 749 292 467 738 726 383 170 162 132 151 960 393 707 673 689 299 483 499\n",
      " 141 362 772 112 665 848 359 415 449 999 124 493 991 713 397 636 968 740\n",
      " 509 391   3 129  25 881 533 899 754 561 948 190 919 528 480 551 529 889\n",
      " 555 766 799 471 211 293 306 419 271 972 400 932 371 618 125 742 865 714\n",
      " 346 450  94 807 753 777 442 947  89 495 119 157 927 604 564 530 482 980\n",
      " 810 294 588 660 734 542 120  34 138 160 161 236 712 401  70 603  10 926\n",
      "  81 970 974 998   4 116 910 106 556 961 637 210 958 534 246 568 375 944\n",
      " 444  55 845 180 366 321 277 609 489 859  98 414 545 233 683 222 757 133\n",
      " 655 791 261 353 912 955 872 809 253  56  45 166 984 417 270 853 652 696\n",
      " 512   9 830 990 598  66 152 176 871 494 104 803 389 863  26 402  82 662\n",
      " 982 584 398 485 521 283 567 155  59 788 143 611 962 333 589 159 275 596\n",
      " 630  31 612 929 209 737 403 503 649 228 629 764 890 527 518 641  68 314\n",
      " 250 330  46 937 365 541 816 434 504 846 818 573 699 394 840 242 744 247\n",
      " 821 994 898 406 645 900 337  13 212 605 800 496 341  54 752 967 678 178\n",
      " 171 566 908 156 418 205 179 441   5  76 965 126 122 519 506 891 620 615\n",
      " 679 410 420 986 558 786 963 486 580 464 920 540 725 778 474 634 342 638\n",
      " 554 203 311  99 760 971 492 606 964 144 949 304 350 514 223  88 739 447\n",
      " 256 741 804 343 671 997 795 476 232 597 773 511 702  85 987  27 385 278\n",
      " 616 586 404 592 574  95 686 249 930 711 940 624 825 117  30 578 759 847\n",
      " 837 461 336 938  58 774 977 839 465 137 897 150 214  64 931 761 681 303\n",
      "  97 544 118  69 456  40 466 731 103 769 331 300 644 517  17 729 478 374\n",
      " 841 969 813 782 939 305 428 610 462 102 894 413 755 870  74 248 335 639\n",
      " 650 163  28 756 440 288 430 272 295 868 448 421 817 697 526 520 560 789\n",
      " 562 976 507 657 469  61 625 380 484 577 832 831 165 758 241 640 903 583\n",
      " 169 975 587 349 710 229 715 316 922 500 260 785 746 252 244  48 522 327\n",
      " 844  19 553 921 225 370 107 666 174 183 472 276 550 866 265 885 134 733\n",
      "  90 720 547  96   7 883  79 856  50 836 623 627 698 470  86 219 917 770\n",
      " 635 101  37 646 684 617 322 779 436 787 302 309 952 548 358   8 408 435\n",
      " 446 633 806 805  29 502 911 918 361 607 187  71 559 198 463 775 290 195\n",
      " 907 181 928 325  23   2 621  87 259 685 323 139 386 115 815 368  60 217\n",
      " 887  36 736   0 675 983 381 849 957 826 796  52 445 585 886 732 842 552\n",
      " 850 426 643 843 893 985 105 716 642 168  11 661 648 458 145 424 188 347\n",
      "  51 798 828 942 622 700 950 437 670 956  57 996 904 854 140  63 193 194\n",
      " 376 572 468 186 860 945 600 213 565 384]\n",
      "['marry', 'me', '.', '<PAD>']\n",
      "['<sos>', 'epouse', 'moi', '!', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['epouse', 'moi', '!', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "\n",
    "# 데이터 섞기\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "print([index_to_src[word] for word in encoder_input[5]])\n",
    "print([index_to_tar[word] for word in decoder_input[5]])\n",
    "print([index_to_tar[word] for word in decoder_target[5]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 100\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(num_samples*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (900, 4)\n",
      "훈련 target 데이터의 크기 : (900, 10)\n",
      "훈련 target 레이블의 크기 : (900, 10)\n",
      "테스트 source 데이터의 크기 : (100, 4)\n",
      "테스트 target 데이터의 크기 : (100, 10)\n",
      "테스트 target 레이블의 크기 : (100, 10)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[데이터로더](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[모델링](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 구조\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embedding_size, hidden_size):\n",
    "        super(Encoder, self).__init__() # 부모 클래스의 생성자 호출\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape == (batch,time,embedding) \n",
    "        x = self.embedding(x)\n",
    "        # hidden.shape == (1,batch,hidden), cell.shape == (1,batch,hidden)\n",
    "        _, (hidden,cell) = self.lstm(x)  # (전체시점 히든, (마지막시점 히든,마지막시점 셀))\n",
    "        # 인코더 출력은 hidden state, cell state\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tar_vocab_size, embedding_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(tar_vocab_size, embedding_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, tar_vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x.shape == (batch,time,embedding)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 디코더의 LSTM으로 인코더의 hidden state, cell state 전달\n",
    "        # output.shape == (batch,time,hidden)\n",
    "        # hidden.shape == (1,batch,hidden) 마지막시점이라서 T=1\n",
    "        # cell.shape == (1,batch,hidden)   마지막시점이라서 T=1\n",
    "        output, (hidden,cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "        # output.shape == (batch,time,vocab)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # 디코더의 출력은 예측값, hidden state, cell state\n",
    "        return output, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # 학습 중에는 디코더의 출력 중 오직 output만 사용\n",
    "        # 디코더 입력과정에서 실제값(정답)을 사용하기 때문에\n",
    "        # lstm계층에서 hidden state와 cell state를 전파할 필요 없음\n",
    "        # 반면, 실제 모델사용할때는 \n",
    "        # 디코더 입력과정에서 이전 디코더의 예측값을 사용하기 때문에\n",
    "        # lstm계층에서 hidden state와 cell state를 계속 흘려야 함\n",
    "        output, _, _ = self.decoder(trg, hidden, cell)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(292, 256, padding_idx=0)\n",
      "    (lstm): LSTM(256, 256, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(635, 256, padding_idx=0)\n",
      "    (lstm): LSTM(256, 256, batch_first=True)\n",
      "    (fc): Linear(in_features=256, out_features=635, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델선언\n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "\n",
    "encoder= Encoder(src_vocab_size, embedding_size, hidden_size)\n",
    "decoder= Decoder(tar_vocab_size, embedding_size, hidden_size)\n",
    "model = Seq2Seq(encoder,decoder)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# ignore_index=0 : 손실 계산 시 특정 레이블(0인 클래스)을 무시\n",
    "# 예) 정답 레이블이 [1, 0, 2, 0]이고, 모델의 예측이 [0.2, 0.5, 0.1, 0.4]일 때, \n",
    "# 0으로 패딩된 위치는 손실 계산에 포함되지 않음\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(292, 256, padding_idx=0)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(635, 256, padding_idx=0)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "    (fc): Linear(in_features=256, out_features=635, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 설정\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, loss, device):\n",
    "    # 평가모드 설정\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad(): # 기울기 계산 비활성화\n",
    "        for encoder_input, decoder_input, decoder_target in dataloader:\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_target = decoder_target.to(device)\n",
    "\n",
    "            # 순방향 전파\n",
    "            # output.shape == (batch, time, tar_vocab)\n",
    "            output = model(encoder_input, decoder_input)\n",
    "\n",
    "            # 손실 계산\n",
    "            # shape 변환: 예측값.shape => 2D (batch x time, vocab) 각 vacob의 확률값\n",
    "            # shape 변환: 실제값.shape => 1D ( ,batch x time) 정수값\n",
    "            # ????? 손실계산 방법이 잘 이해XXX??????????????\n",
    "            loss = loss_function(output.view(-1,output.size(-1)), decoder_target.view(-1)) \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산(패딩 토큰 제외)\n",
    "            mask = decoder_target != 0\n",
    "            total_correct += ((output.argmax(axis=-1) == decoder_target) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "    \n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count\n",
    "    # 평균손실은 총손실은 데이터로더의 배치수로 나눈다\n",
    "    # 정확도는 총정확도를 총 토큰 수로 나눈다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Train Loss: 2.3009 | Train Acc: 0.5356 | Valid Loss: 2.8202 | Valid Acc: 0.5252\n",
      "Validation loss improved from inf to 2.8202. 체크포인트를 저장합니다.\n",
      "Epoch: 2/10 | Train Loss: 2.2628 | Train Acc: 0.5562 | Valid Loss: 2.7462 | Valid Acc: 0.5468\n",
      "Validation loss improved from 2.8202 to 2.7462. 체크포인트를 저장합니다.\n",
      "Epoch: 3/10 | Train Loss: 2.1227 | Train Acc: 0.5550 | Valid Loss: 2.6938 | Valid Acc: 0.5468\n",
      "Validation loss improved from 2.7462 to 2.6938. 체크포인트를 저장합니다.\n",
      "Epoch: 4/10 | Train Loss: 2.0346 | Train Acc: 0.5712 | Valid Loss: 2.6424 | Valid Acc: 0.5492\n",
      "Validation loss improved from 2.6938 to 2.6424. 체크포인트를 저장합니다.\n",
      "Epoch: 5/10 | Train Loss: 1.9607 | Train Acc: 0.5854 | Valid Loss: 2.5915 | Valid Acc: 0.5588\n",
      "Validation loss improved from 2.6424 to 2.5915. 체크포인트를 저장합니다.\n",
      "Epoch: 6/10 | Train Loss: 1.8413 | Train Acc: 0.6073 | Valid Loss: 2.5309 | Valid Acc: 0.5683\n",
      "Validation loss improved from 2.5915 to 2.5309. 체크포인트를 저장합니다.\n",
      "Epoch: 7/10 | Train Loss: 1.7480 | Train Acc: 0.6174 | Valid Loss: 2.4904 | Valid Acc: 0.5731\n",
      "Validation loss improved from 2.5309 to 2.4904. 체크포인트를 저장합니다.\n",
      "Epoch: 8/10 | Train Loss: 1.7739 | Train Acc: 0.6326 | Valid Loss: 2.4593 | Valid Acc: 0.5803\n",
      "Validation loss improved from 2.4904 to 2.4593. 체크포인트를 저장합니다.\n",
      "Epoch: 9/10 | Train Loss: 1.5829 | Train Acc: 0.6380 | Valid Loss: 2.4385 | Valid Acc: 0.5779\n",
      "Validation loss improved from 2.4593 to 2.4385. 체크포인트를 저장합니다.\n",
      "Epoch: 10/10 | Train Loss: 1.4893 | Train Acc: 0.6533 | Valid Loss: 2.3921 | Valid Acc: 0.5923\n",
      "Validation loss improved from 2.4385 to 2.3921. 체크포인트를 저장합니다.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 훈련모드 설정\n",
    "    model.train()\n",
    "\n",
    "    for encoder_input, decoder_input, decoder_target in train_dataloader:\n",
    "        encoder_input = encoder_input.to(device)\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        decoder_target = decoder_target.to(device)\n",
    "\n",
    "        # 순전파\n",
    "        output = model(encoder_input, decoder_input)\n",
    "\n",
    "        # 손실\n",
    "        loss = loss_function(output.view(-1, output.size(-1)), decoder_target.view(-1))\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # 평가\n",
    "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if valid_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[모델 로딩](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 2.3921\n",
      "Best model validation accuracy: 0.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_8616\\2588588691.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
    "model.to(device)\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tar_vocab['<sos>'])\n",
    "print(tar_vocab['<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[동작시키기 (test 모드)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 22  2  0]\n",
      "[  3  56 168   5   0   0   0   0   0   0]\n",
      "[ 56 168   5   4   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다름\n",
    "# 그래서 테스트 과정을 위해 모델을 다시 설계(디코더 수정)\n",
    "\n",
    "# 1) 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻는다\n",
    "# 2) 인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 전달\n",
    "# 3) 디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복\n",
    "\n",
    "index_to_src = {i: w for w, i in src_vocab.items()}\n",
    "index_to_tar = {i: w for w, i in tar_vocab.items()}\n",
    "\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_src[encoded_word]+' '\n",
    "    return sentence\n",
    "\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n",
    "            sentence = sentence + index_to_tar[encoded_word]+' '\n",
    "    return sentence\n",
    "\n",
    "print(encoder_input_test[25])\n",
    "print(decoder_input_test[25])\n",
    "print(decoder_target_test[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 단계에서는 디코더를 매 시점 별로 컨트롤\n",
    "# 사용될 변수는 decoder_input\n",
    "# unsquzzez(dim) : dim번째 차원에 차원추가\n",
    "\n",
    "def decode_sequence(input_seq, model, max_output_len, src_vocab_size, tar_vocab_size, int_to_src, int_to_tar):\n",
    "    # input_seq: 단어 1개의 특징벡터(embedding 벡터)\n",
    "    encoder_input = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device) # 3D (배치1)\n",
    "\n",
    "    # 인코더의 초기 상태 설정\n",
    "    hidden, cell = model.encoder(encoder_input)\n",
    "\n",
    "    # 시작 토큰 <SOS>를 디코더의 첫입력으로 설정\n",
    "    # unsquzzez(0)은 배치 차원 추가 목적\n",
    "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device) # 2D\n",
    "    decoded_tokens = []\n",
    "\n",
    "    # for문을 도는 것 == 디코더의 각 시점\n",
    "    # 디코더의 각 시점마다 이전 예측값을 decoder_input으로 전달\n",
    "    print(\"input_seq: \", input_seq)\n",
    "    for i in range(max_output_len):\n",
    "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "        \n",
    "        # 소프트맥스 회귀. 예측 단어의 인덱스\n",
    "        output_token = output.argmax(dim=-1).item()\n",
    "        \n",
    "        # report\n",
    "        print(\"report=> {}th | decoder_input=> {} |output_token=> {}\".format(i,decoder_input, output_token))\n",
    "        \n",
    "        # 종료 토큰 <eos>\n",
    "        if output_token == 4:\n",
    "            print(\"decoded_tokens: \", decoded_tokens)\n",
    "            break\n",
    "\n",
    "        # 현재 시점의 예측값을 다음 시점의 입력으로 전달\n",
    "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        \n",
    "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴\n",
    "        decoded_tokens.append(output_token)\n",
    "\n",
    "\n",
    "    result = ' '.join(int_to_tar[token] for token in decoded_tokens)\n",
    "    print(\"result: \", result)\n",
    "    print(\"-\"*50) \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_index:  3\n",
      "input_seq:  [ 4 65  2  0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 6\n",
      "report=> 1th | decoder_input=> tensor([[6]]) |output_token=> 7\n",
      "report=> 2th | decoder_input=> tensor([[7]]) |output_token=> 280\n",
      "report=> 3th | decoder_input=> tensor([[280]]) |output_token=> 2\n",
      "report=> 4th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [6, 7, 280, 2]\n",
      "result:  est ? aimes .\n",
      "--------------------------------------------------\n",
      "입력문장 : you know . \n",
      "정답문장 : est ? viens . \n",
      "번역문장 : est ? aimes .\n",
      "==================================================\n",
      "seq_index:  50\n",
      "input_seq:  [ 4 58  5  2]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 6\n",
      "report=> 1th | decoder_input=> tensor([[6]]) |output_token=> 7\n",
      "report=> 2th | decoder_input=> tensor([[7]]) |output_token=> 238\n",
      "report=> 3th | decoder_input=> tensor([[238]]) |output_token=> 2\n",
      "report=> 4th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [6, 7, 238, 2]\n",
      "result:  est ? tes .\n",
      "--------------------------------------------------\n",
      "입력문장 : you please ? . \n",
      "정답문장 : c pas nous ca l vous te . \n",
      "번역문장 : est ? tes .\n",
      "==================================================\n",
      "seq_index:  70\n",
      "input_seq:  [106  14   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 18\n",
      "report=> 1th | decoder_input=> tensor([[18]]) |output_token=> 14\n",
      "report=> 2th | decoder_input=> tensor([[14]]) |output_token=> 2\n",
      "report=> 3th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [18, 14, 2]\n",
      "result:  tu il .\n",
      "--------------------------------------------------\n",
      "입력문장 : hate t . \n",
      "정답문장 : reponse de tu . \n",
      "번역문장 : tu il .\n",
      "==================================================\n",
      "seq_index:  99\n",
      "input_seq:  [18 90  2  0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 63\n",
      "report=> 1th | decoder_input=> tensor([[63]]) |output_token=> 5\n",
      "report=> 2th | decoder_input=> tensor([[5]]) |output_token=> 4\n",
      "decoded_tokens:  [63, 5]\n",
      "result:  besoin je\n",
      "--------------------------------------------------\n",
      "입력문장 : me try . \n",
      "정답문장 : besoin avons ne je \n",
      "번역문장 : besoin je\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터에 대해 샘플 결과 확인\n",
    "\n",
    "for seq_index in [3,50,70,99]: \n",
    "    # -------- 데이터 단위 이해 --------------\n",
    "    # 리스트[]로 묶인 건 문장 단위이고, 리스트 안의 원소는 단어(vocab) 단위\n",
    "    #  ['go', '.'] 같은 문장을 단어로 쪼개어 인덱스화 해서 모은 것\n",
    "\n",
    "    # 여기서 [3,50,100,300]의 각 원소는 사실 문장단위로\n",
    "    # 번역 테스트할 문장을 리스트에 모아놓은 것\n",
    "    # 원소3을 encoder_input_train에서 인덱싱하면 [ 4 65  2  0] 문장!!\n",
    "\n",
    "    # encoder_input_train : 훈련에 사용된 문장들의 집합 (2D배열)\n",
    "    # input_seq: 위 문장집합에서 해당하는 문장 추출한 것 (1D배열)\n",
    "    print(\"seq_index: \", seq_index)\n",
    "    input_seq = encoder_input_train[seq_index] \n",
    "    translated_text = decode_sequence(input_seq, model, 20, src_vocab_size, tar_vocab_size, index_to_src, index_to_tar)\n",
    "\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",translated_text)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_index:  3\n",
      "input_seq:  [18 28  2  0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 63\n",
      "report=> 1th | decoder_input=> tensor([[63]]) |output_token=> 5\n",
      "report=> 2th | decoder_input=> tensor([[5]]) |output_token=> 4\n",
      "decoded_tokens:  [63, 5]\n",
      "result:  besoin je\n",
      "--------------------------------------------------\n",
      "입력문장 : you know . \n",
      "정답문장 : est ? viens . \n",
      "번역문장 : besoin je\n",
      "==================================================\n",
      "seq_index:  50\n",
      "input_seq:  [38 36  3  0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 21\n",
      "report=> 1th | decoder_input=> tensor([[21]]) |output_token=> 12\n",
      "report=> 2th | decoder_input=> tensor([[12]]) |output_token=> 186\n",
      "report=> 3th | decoder_input=> tensor([[186]]) |output_token=> 5\n",
      "report=> 4th | decoder_input=> tensor([[5]]) |output_token=> 4\n",
      "decoded_tokens:  [21, 12, 186, 5]\n",
      "result:  c pas idee je\n",
      "--------------------------------------------------\n",
      "입력문장 : you please ? . \n",
      "정답문장 : c pas nous ca l vous te . \n",
      "번역문장 : c pas idee je\n",
      "==================================================\n",
      "seq_index:  70\n",
      "input_seq:  [ 4  8 76  2]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 6\n",
      "report=> 1th | decoder_input=> tensor([[6]]) |output_token=> 7\n",
      "report=> 2th | decoder_input=> tensor([[7]]) |output_token=> 238\n",
      "report=> 3th | decoder_input=> tensor([[238]]) |output_token=> 2\n",
      "report=> 4th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [6, 7, 238, 2]\n",
      "result:  est ? tes .\n",
      "--------------------------------------------------\n",
      "입력문장 : hate t . \n",
      "정답문장 : reponse de tu . \n",
      "번역문장 : est ? tes .\n",
      "==================================================\n",
      "seq_index:  99\n",
      "input_seq:  [ 7 17  2  0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 31\n",
      "report=> 1th | decoder_input=> tensor([[31]]) |output_token=> 5\n",
      "report=> 2th | decoder_input=> tensor([[5]]) |output_token=> 4\n",
      "decoded_tokens:  [31, 5]\n",
      "result:  me je\n",
      "--------------------------------------------------\n",
      "입력문장 : me try . \n",
      "정답문장 : besoin avons ne je \n",
      "번역문장 : me je\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 테스트트데이터에 대해 샘플 결과 확인\n",
    "\n",
    "for seq_index in [3,50,70,99]: \n",
    "    # -------- 데이터 단위 이해 --------------\n",
    "    # 리스트[]로 묶인 건 문장 단위이고, 리스트 안의 원소는 단어(vocab) 단위\n",
    "    #  ['go', '.'] 같은 문장을 단어로 쪼개어 인덱스화 해서 모은 것\n",
    "\n",
    "    # 여기서 [3,50,100,300]의 각 원소는 사실 문장단위로\n",
    "    # 번역 테스트할 문장을 리스트에 모아놓은 것\n",
    "    # 원소3을 encoder_input_train에서 인덱싱하면 [ 4 65  2  0] 문장!!\n",
    "\n",
    "    # encoder_input_train : 훈련에 사용된 문장들의 집합 (2D배열)\n",
    "    # input_seq: 위 문장집합에서 해당하는 문장 추출한 것 (1D배열)\n",
    "    print(\"seq_index: \", seq_index)\n",
    "    input_seq = encoder_input_test[seq_index] \n",
    "    translated_text = decode_sequence(input_seq, model, 20, src_vocab_size, tar_vocab_size, index_to_src, index_to_tar)\n",
    "\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",translated_text)\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH38_CPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
