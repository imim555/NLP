{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [seq2se2 실습](#toc1_1_)    \n",
    "  - [데이터 로딩](#toc1_2_)    \n",
    "  - [전처리](#toc1_3_)    \n",
    "  - [데이터 변환](#toc1_4_)    \n",
    "  - [데이터 분리 : train & test](#toc1_5_)    \n",
    "  - [데이터로더](#toc1_6_)    \n",
    "  - [모델링](#toc1_7_)    \n",
    "  - [모델 로딩](#toc1_8_)    \n",
    "  - [동작시키기 (test 모드)](#toc1_9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[seq2se2 실습](#toc0_)\n",
    "- 영어->프랑스어 변역기\n",
    "- python -v 3.8.19\n",
    "- pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[데이터 로딩](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "# User-Agent => Python의 기본 requests 요청을 차단 방지\n",
    "# timeout => 네트워크 문제로 인한 응답 지연 확인\n",
    "# 파일을 wb(바이너리 모드)로 저장해서 손상 방지\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "filename = \"./attach/fra-eng.zip\"\n",
    "\n",
    "# User-Agent 설정\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# 파일 다운로드\n",
    "response = requests.get(url, headers=headers, stream=True, timeout=10)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(filename, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "    print(\"✅ Download complete!\")\n",
    "else:\n",
    "    print(f\"❌ 다운로드 실패: HTTP {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip complete!\n"
     ]
    }
   ],
   "source": [
    "# 압축 풀기\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "with zipfile.ZipFile(\"./attach/fra-eng.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"attach\")  # 원하는 경로로 변경 가능\n",
    "\n",
    "print(\"Unzip complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[전처리](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    # 프랑스어 악센트 삭제\n",
    "    # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 악센트 삭제 함수 호출\n",
    "    sent=unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백 생성\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "def load_preprocessed_data():\n",
    "  encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "  with open(\"./attach/fra.txt\", \"r\") as lines:\n",
    "    for i, line in enumerate(lines):\n",
    "      # source 데이터와 target 데이터 분리\n",
    "      src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "      # source 데이터 전처리\n",
    "      src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "      \n",
    "      # target 데이터 전처리\n",
    "      tar_line = preprocess_sentence(tar_line)\n",
    "      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "      encoder_input.append(src_line)\n",
    "      decoder_input.append(tar_line_in)\n",
    "      decoder_target.append(tar_line_out)\n",
    "\n",
    "      if i == num_samples - 1:\n",
    "        break\n",
    "\n",
    "  return encoder_input, decoder_input, decoder_target\n",
    "# 디코더 입력 데이터를 생성한 이유\n",
    "# 모델구조는 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다\n",
    "# 하지만, 학습과정에서는 학습 효율을 높이기 위해 이전 디코더 셀의 예측값 대신 실제값을 넣어주므로\n",
    "# 별로도 디코터 입력 데이터도 만든 것이다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "# ['go', '.'] => 문장1개로 간주하자\n",
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[데이터 변환](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어집합 생성\n",
    "# -----------------\n",
    "# word <-> index\n",
    "# 빈도순으로 정수 정렬\n",
    "# 0번<PAD>, 1번<UNK>, 2번<빈도수 가장 많은 단어> ...\n",
    "\n",
    "def build_vocab(sents):\n",
    "    word_list = []\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word_list.append(word)\n",
    "    # 빈도수 계산 및 정렬\n",
    "    word_counts = Counter(word_list)\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    word_to_index={}\n",
    "    word_to_index['<PAD>']=0\n",
    "    word_to_index['<UNK>']=1\n",
    "    for index, word in enumerate(vocab):\n",
    "        word_to_index[word] = index+2\n",
    "\n",
    "    return word_to_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 292, 프랑스어 단어 집합의 크기 : 635\n",
      "{0: '<PAD>', 1: '<UNK>', 2: '.', 3: '!', 4: 'i', 5: 'it', 6: 'get', 7: 'go', 8: 'm', 9: 'away', 10: 'out', 11: 'me', 12: '?', 13: 'up', 14: 'tom', 15: 'be', 16: 'lost', 17: 'on', 18: 'come', 19: 'ahead', 20: 'we', 21: 'run', 22: 'down', 23: 's', 24: 'beat', 25: 'won', 26: 'terrific', 27: 'you', 28: 'back', 29: 'this', 30: 'try', 31: 'no', 32: 'who', 33: 'relax', 34: 'way', 35: 'fair', 36: 'nice', 37: 'us', 38: 'how', 39: 'look', 40: 'help', 41: 'hold', 42: 'calm', 43: 'off', 44: 'forget', 45: 'see', 46: 'got', 47: 'him', 48: 'he', 49: 'shut', 50: 'fun', 51: 'leave', 52: 'don', 53: 't', 54: 'stop', 55: 'wait', 56: 'call', 57: 'take', 58: 'did', 59: 'use', 60: 'sit', 61: 'excuse', 62: 'hello', 63: 'now', 64: 'in', 65: 'left', 66: 'paid', 67: 'home', 68: 'hang', 69: 'll', 70: 'kill', 71: 'open', 72: 'am', 73: 'brief', 74: 'good', 75: 'hurry', 76: 'ok', 77: 'really', 78: 'ask', 79: 'cool', 80: 'goodbye', 81: 'sad', 82: 'drive', 83: 'push', 84: 'wake', 85: 'grab', 86: 'let', 87: 'what', 88: 'trust', 89: 'catch', 90: 'over', 91: 'attack', 92: 'buy', 93: 'cheers', 94: 'thanks', 95: 'awesome', 96: 'burn', 97: 'bury', 98: 'drop', 99: 'fold', 100: 'hire', 101: 'fat', 102: 'hit', 103: 'keep', 104: 'lie', 105: 'lock', 106: 'tell', 107: 'win', 108: 'can', 109: 'job', 110: 'have', 111: 'cute', 112: 'lazy', 113: 'sure', 114: 'she', 115: 'here', 116: 'taste', 117: 'they', 118: 'touch', 119: 'watch', 120: 'after', 121: 'feel', 122: 'hi', 123: 'wow', 124: 'duck', 125: 'smile', 126: 'hug', 127: 'fell', 128: 'know', 129: 'quit', 130: 'tried', 131: 'mad', 132: 'wet', 133: 'join', 134: 'kiss', 135: 'too', 136: 'perfect', 137: 'aim', 138: 'still', 139: 'spoke', 140: 'goofed', 141: 'fine', 142: 'free', 143: 'late', 144: 'ugly', 145: 'may', 146: 'came', 147: 'died', 148: 'speak', 149: 'some', 150: 'honest', 151: 'seated', 152: 'fantastic', 153: 'follow', 154: 'fire', 155: 'hide', 156: 'jump', 157: 'begin', 158: 'eat', 159: 'hop', 160: 'lied', 161: 'slow', 162: 'runs', 163: 'agree', 164: 'dozed', 165: 'froze', 166: 'stood', 167: 'swore', 168: 'kick', 169: 'low', 170: 'move', 171: 'pull', 172: 'show', 173: 'sign', 174: 'skip', 175: 'wash', 176: 'welcome', 177: 'high', 178: 'them', 179: 'a', 180: 'man', 181: 'cheer', 182: 'find', 183: 'fix', 184: 'real', 185: 'phoned', 186: 'refuse', 187: 'rested', 188: 'saw', 189: 'stayed', 190: 'pay', 191: 'busy', 192: 'deaf', 193: 'full', 194: 'game', 195: 'okay', 196: 'tidy', 197: 'well', 198: 've', 199: 'works', 200: 'his', 201: 'new', 202: 'marry', 203: 'prove', 204: 'save', 205: 'speed', 206: 'warn', 207: 'for', 208: 'write', 209: 'chill', 210: 'soon', 211: 'dogs', 212: 'bark', 213: 'die', 214: 'film', 215: 'oh', 216: 'sorry', 217: 'exhale', 218: 'fled', 219: 'hunt', 220: 'knit', 221: 'pass', 222: 'swim', 223: 'inhale', 224: 'listen', 225: 'kind', 226: 'cried', 227: 'drove', 228: 'fired', 229: 'smoke', 230: 'snore', 231: 'stink', 232: 'waved', 233: 'fit', 234: 'ill', 235: 'shy', 236: 'pair', 237: 'so', 238: 'long', 239: 'care', 240: 'ran', 241: 'brave', 242: 'buzz', 243: 'cover', 244: 'cuff', 245: 'tries', 246: 'guys', 247: 'deep', 248: 'rude', 249: 'wise', 250: 'cursed', 251: 'failed', 252: 'forgot', 253: 'helped', 254: 'jumped', 255: 'looked', 256: 'moaned', 257: 'nodded', 258: 'obeyed', 259: 'sighed', 260: 'smiled', 261: 'talked', 262: 'bald', 263: 'cold', 264: 'done', 265: 'fast', 266: 'glad', 267: 'rich', 268: 'safe', 269: 'sick', 270: 'tall', 271: 'thin', 272: 'weak', 273: 'helps', 274: 'hurts', 275: 'hot', 276: 'odd', 277: 'red', 278: 'say', 279: 'stand', 280: 'knew', 281: 'lies', 282: 'went', 283: 're', 284: 'answer', 285: 'strong', 286: 'birds', 287: 'fly', 288: 'bless', 289: 'choose', 290: 'do', 291: 'cry'}\n"
     ]
    }
   ],
   "source": [
    "# word -> index \n",
    "src_to_index = build_vocab(sents_en_in)\n",
    "tar_to_index = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_to_index)\n",
    "tar_vocab_size = len(tar_to_index)\n",
    "\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n",
    "\n",
    "\n",
    "# index -> word\n",
    "index_to_src = {idx:word for word,idx in src_to_index.items() }\n",
    "index_to_tar = {idx:word for word,idx in tar_to_index.items() }\n",
    "print(index_to_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수인코딩\n",
    "\n",
    "def texts_to_sequences(sents, word_to_index):\n",
    "    encoded_X_data = []\n",
    "    for sent in tqdm(sents):\n",
    "        index_sequences = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                index_sequences.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                index_sequences.append(word_to_index['<UNK>'])\n",
    "        encoded_X_data.append(index_sequences)\n",
    "    return encoded_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 460507.69it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 904529.65it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 984116.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [7, 2]\n",
      "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [122, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input = texts_to_sequences(sents_en_in, src_to_index)\n",
    "decoder_input = texts_to_sequences(sents_fra_in, tar_to_index)\n",
    "decoder_target = texts_to_sequences(sents_fra_out, tar_to_index)\n",
    "\n",
    "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
    "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
    "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
    "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "# 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
    "def pad_sequences(sentences, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence)!=0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (1000, 4)\n",
      "디코더의 입력의 크기(shape) : (1000, 10)\n",
      "디코더의 레이블의 크기(shape) : (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input)\n",
    "decoder_input = pad_sequences(decoder_input)\n",
    "decoder_target = pad_sequences(decoder_target)\n",
    "\n",
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[데이터 분리 : train & test](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [498  83 403 123 298 169  91 839 318 970 361 161 446 663 617 951 644 576\n",
      " 860 883  40 864 815 140 261 151 335 439 499 534 459 481  84  28 639 506\n",
      " 952 445 693 433 956 157 147 101  67 237 637 184 525 973 796  58 721 932\n",
      " 172 175 602 665  76 280 792 819 155 892 756 824 806 723 284 670  75 369\n",
      " 548 142 319 366 825 278 156 759 251 505 255 642 501  66 372 356 376  22\n",
      " 486 881 735 712 734 465 544 696 204 874 223 804 520 180 305 390 222 803\n",
      " 885   1 423 504  95 758 206  27 540  48 209 490 492 488  35 441 386 457\n",
      " 777 272 711  59 776 573 160 473 943 391  19 610  47  94 256 477 683 333\n",
      " 733 958 782 914 771 246 516 212 969 182 826 148 364 886 307 808 676 852\n",
      " 722 347 717 991 672 558 719 997 349 300 828 330  72 210  50 744 352  77\n",
      " 774 944 177 235 196 800 311 884 303 994 757  11  45 961 456 560 811 679\n",
      " 833 873 383 487 813  18 461 135 414 917  21 580 786 664 613 767 764  93\n",
      " 768 667 596 408 217 468 750 628 578 264 902 502 341  89 214 631 130 221\n",
      " 779 281 922  98 569 714 497 437 344 791 749 570  90 654  63 962 337 287\n",
      " 838 149 106 701 313  24 378 116 104 168 483 242 396 702 360 583 770 218\n",
      " 241 720 144 948 550 889 603 348 133 876 285 185 402 228 183 247 965 551\n",
      " 823 100 370 166 703 668 935 125 716 291 342 784 299 928 200  71 650 730\n",
      " 388 620 766 963 875 984 170 995 609 959 553 528 612 940 894 412 282 353\n",
      "  23 290 724 880 192 634 112 496 794 413 657 193 127 896 587 306 649 537\n",
      " 938 154 339 765  46 405 399 485 132 189 915 802 238 145  86 594 622 999\n",
      " 967  38 589 252 312 941 772 621 861 865 809 126 176 856 340 862 332 964\n",
      " 198 128 987 179 503   3  39 829 908 536 996 267 472 891 475 110 912  17\n",
      " 362 632 638 320 368 834 178 220 688 581 718 404 406 294 990 407 710 787\n",
      " 531 957 728 230 379 310 248 357 479 872 205   8 409 137 257 978 297 677\n",
      " 835 630 704 562 934 495 354 336 812 859 427 662 643 989 561 565 737 947\n",
      " 745 982 611 748 556 426  61 849 152  79 725 821 816 597 302 328 900 117\n",
      " 760 568 226  49  52 778 697 857 635 845 277 929 321 671 263 323 118 615\n",
      "   7 746 139 452 392 476 755 338  74 535 681  57 334 416 740 626 373 927\n",
      " 454 181 801 729 946 855 781   5 211 395 993   4 511 119 979 225  37 751\n",
      "  78 968 438 988 694  70 541 678 707 239 585 731 309  54 555   2  73 422\n",
      " 474  43 653 141 669 153 674 397 797 530  44 478 308 385 227 559 992 742\n",
      " 584 933 588 359 607 888 274 601 111 920 136 741 296 913 432 331 380 769\n",
      "  56 986 572  16 863 870 789 268 363 514 887 659 700 675 545 753 736 190\n",
      "  64 830 253 480 325 134 810 115 673 971 418 624 199 419 910 906 453 515\n",
      " 377 512 275 384  12 909 625 651 939 606 832 571  68  87 471 799 788 121\n",
      " 219 174 547 599  41 614 848 901 976 146 854  33 763 203 113 424 591 586\n",
      " 435 709 521 343  92 985 270 713 293 818 687 447 715 262 842 898 400 188\n",
      "  15 428 229 107 224 269 656 604  13 798  32 623 164 165 577 448 598 232\n",
      " 552 271 866 187 417 726 640 533  80 367 150 216  53 851 523 647  88 844\n",
      " 440 415 627 785 775 930 871 449 895  82   0 114 233 345 265 143 699 494\n",
      " 527 129 950 102 288 346 919 783 877 817 163 972  26 648 907 539 159 295\n",
      " 752 442 899 517 878 276 836 243 463 460 444 108 942  20 738 507 554 557\n",
      "  51  31 234 532 425 322 685 314 167 837 138 732 122   6 109 304 868 890\n",
      " 998 937 814 317 464 790 375 524 316 103 371 546 954 822 743 522 526 921\n",
      " 955 931 773 292  96 564 458 289 351 249 207 605 960 283  62 574 686 579\n",
      " 974 727 641 918 470 120 244 186 682 706 692 652  85 215 266 882 936 905\n",
      " 567 754 680  60 916 194 592 201 618 867 590 739 350 191 387 254 608 508\n",
      " 258 105 315 645  69 566  10 518 761 398 619 949 925 636 542  30  97 629\n",
      " 840 926 326 538 431 807 666 401 513 455 593 595 173 421  42 869  65 469\n",
      " 695  36 633 365 846 451 646 691 519 213 708 911 162 208 945  81 240 897\n",
      " 793 966 953 924 411 689 690 393 705 509 273 429   9 279 698 983 747 245\n",
      " 500 549 301 286 131 980 374  34 575  29 231 493 327 382 394  99 795 827\n",
      " 780 543 903 466 355 616 250 410 661 443 655 831 434 436 197 658 879 843\n",
      " 975  25 430 324 329 482 389 467 904 820  55  14 462 381 236 124 853 450\n",
      " 484 841 358 858 202 684 923 420 582 660 847 805 563 981 510 529 489 171\n",
      " 259 762 893 195 158 491 600 260 977 850]\n",
      "['be', 'calm', '.', '<PAD>']\n",
      "['<sos>', 'soyez', 'calme', '!', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['soyez', 'calme', '!', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "\n",
    "# 데이터 섞기\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "print([index_to_src[word] for word in encoder_input[5]])\n",
    "print([index_to_tar[word] for word in decoder_input[5]])\n",
    "print([index_to_tar[word] for word in decoder_target[5]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 100\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(num_samples*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (900, 4)\n",
      "훈련 target 데이터의 크기 : (900, 10)\n",
      "훈련 target 레이블의 크기 : (900, 10)\n",
      "테스트 source 데이터의 크기 : (100, 4)\n",
      "테스트 target 데이터의 크기 : (100, 10)\n",
      "테스트 target 레이블의 크기 : (100, 10)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[데이터로더](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[모델링](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 구조\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embedding_size, hidden_size):\n",
    "        super(Encoder, self).__init__() # 부모 클래스의 생성자 호출\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape == (batch,time,embedding) \n",
    "        x = self.embedding(x)\n",
    "        # hidden.shape == (1,batch,hidden), cell.shape == (1,batch,hidden)\n",
    "        _, (hidden,cell) = self.lstm(x)  # (전체시점 히든, (마지막시점 히든,마지막시점 셀))\n",
    "        # 인코더 출력은 hidden state, cell state\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tar_vocab_size, embedding_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(tar_vocab_size, embedding_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, tar_vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x.shape == (batch,time,embedding)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 디코더의 LSTM으로 인코더의 hidden state, cell state 전달\n",
    "        # output.shape == (batch,time,hidden)\n",
    "        # hidden.shape == (1,batch,hidden) 마지막시점이라서 T=1\n",
    "        # cell.shape == (1,batch,hidden)   마지막시점이라서 T=1\n",
    "        output, (hidden,cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "        # output.shape == (batch,time,vocab)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        # 디코더의 출력은 예측값, hidden state, cell state\n",
    "        return output, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # 학습 중에는 디코더의 출력 중 오직 output만 사용\n",
    "        # 디코더 입력과정에서 실제값(정답)을 사용하기 때문에\n",
    "        # lstm계층에서 hidden state와 cell state를 전파할 필요 없음\n",
    "        # 반면, 실제 모델사용할때는 \n",
    "        # 디코더 입력과정에서 이전 디코더의 예측값을 사용하기 때문에\n",
    "        # lstm계층에서 hidden state와 cell state를 계속 흘려야 함\n",
    "        output, _, _ = self.decoder(trg, hidden, cell)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(292, 256, padding_idx=0)\n",
      "    (lstm): LSTM(256, 256, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(635, 256, padding_idx=0)\n",
      "    (lstm): LSTM(256, 256, batch_first=True)\n",
      "    (fc): Linear(in_features=256, out_features=635, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델선언\n",
    "embedding_size = 256\n",
    "hidden_size = 256\n",
    "\n",
    "encoder= Encoder(src_vocab_size, embedding_size, hidden_size)\n",
    "decoder= Decoder(tar_vocab_size, embedding_size, hidden_size)\n",
    "model = Seq2Seq(encoder,decoder)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# ignore_index=0 : 손실 계산 시 특정 레이블(0인 클래스)을 무시\n",
    "# 예) 정답 레이블이 [1, 0, 2, 0]이고, 모델의 예측이 [0.2, 0.5, 0.1, 0.4]일 때, \n",
    "# 0으로 패딩된 위치는 손실 계산에 포함되지 않음\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(292, 256, padding_idx=0)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(635, 256, padding_idx=0)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "    (fc): Linear(in_features=256, out_features=635, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 설정\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, loss, device):\n",
    "    # 평가모드 설정\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad(): # 기울기 계산 비활성화\n",
    "        for encoder_input, decoder_input, decoder_target in dataloader:\n",
    "            encoder_input = encoder_input.to(device)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            decoder_target = decoder_target.to(device)\n",
    "\n",
    "            # 순방향 전파\n",
    "            # output.shape == (batch, time, tar_vocab)\n",
    "            output = model(encoder_input, decoder_input)\n",
    "\n",
    "            # 손실 계산\n",
    "            # shape 변환: 예측값.shape => 2D (batch x time, vocab) 각 vacob의 확률값\n",
    "            # shape 변환: 실제값.shape => 1D ( ,batch x time) 정수값\n",
    "            # ????? 손실계산 방법이 잘 이해XXX??????????????\n",
    "            loss = loss_function(output.view(-1,output.size(-1)), decoder_target.view(-1)) \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산(패딩 토큰 제외)\n",
    "            mask = decoder_target != 0\n",
    "            total_correct += ((output.argmax(axis=-1) == decoder_target) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "    \n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count\n",
    "    # 평균손실은 총손실은 데이터로더의 배치수로 나눈다\n",
    "    # 정확도는 총정확도를 총 토큰 수로 나눈다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 | Train Loss: 2.2619 | Train Acc: 0.5521 | Valid Loss: 2.9940 | Valid Acc: 0.4964\n",
      "Validation loss improved from inf to 2.9940. 체크포인트를 저장합니다.\n",
      "Epoch: 2/10 | Train Loss: 2.1673 | Train Acc: 0.5750 | Valid Loss: 2.9371 | Valid Acc: 0.5206\n",
      "Validation loss improved from 2.9940 to 2.9371. 체크포인트를 저장합니다.\n",
      "Epoch: 3/10 | Train Loss: 1.9995 | Train Acc: 0.5781 | Valid Loss: 2.8878 | Valid Acc: 0.5157\n",
      "Validation loss improved from 2.9371 to 2.8878. 체크포인트를 저장합니다.\n",
      "Epoch: 4/10 | Train Loss: 1.9576 | Train Acc: 0.5889 | Valid Loss: 2.8361 | Valid Acc: 0.5327\n",
      "Validation loss improved from 2.8878 to 2.8361. 체크포인트를 저장합니다.\n",
      "Epoch: 5/10 | Train Loss: 1.7623 | Train Acc: 0.6070 | Valid Loss: 2.7838 | Valid Acc: 0.5424\n",
      "Validation loss improved from 2.8361 to 2.7838. 체크포인트를 저장합니다.\n",
      "Epoch: 6/10 | Train Loss: 1.7178 | Train Acc: 0.6211 | Valid Loss: 2.7436 | Valid Acc: 0.5448\n",
      "Validation loss improved from 2.7838 to 2.7436. 체크포인트를 저장합니다.\n",
      "Epoch: 7/10 | Train Loss: 1.6908 | Train Acc: 0.6348 | Valid Loss: 2.6952 | Valid Acc: 0.5521\n",
      "Validation loss improved from 2.7436 to 2.6952. 체크포인트를 저장합니다.\n",
      "Epoch: 8/10 | Train Loss: 1.5378 | Train Acc: 0.6482 | Valid Loss: 2.6568 | Valid Acc: 0.5593\n",
      "Validation loss improved from 2.6952 to 2.6568. 체크포인트를 저장합니다.\n",
      "Epoch: 9/10 | Train Loss: 1.4931 | Train Acc: 0.6593 | Valid Loss: 2.6138 | Valid Acc: 0.5690\n",
      "Validation loss improved from 2.6568 to 2.6138. 체크포인트를 저장합니다.\n",
      "Epoch: 10/10 | Train Loss: 1.3938 | Train Acc: 0.6732 | Valid Loss: 2.5940 | Valid Acc: 0.5642\n",
      "Validation loss improved from 2.6138 to 2.5940. 체크포인트를 저장합니다.\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 훈련모드 설정\n",
    "    model.train()\n",
    "\n",
    "    for encoder_input, decoder_input, decoder_target in train_dataloader:\n",
    "        encoder_input = encoder_input.to(device)\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        decoder_target = decoder_target.to(device)\n",
    "\n",
    "        # 순전파\n",
    "        output = model(encoder_input, decoder_input)\n",
    "\n",
    "        # 손실\n",
    "        loss = loss_function(output.view(-1, output.size(-1)), decoder_target.view(-1))\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # 평가\n",
    "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if valid_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './attach/best_model_checkpoint.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[모델 로딩](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 2.5940\n",
      "Best model validation accuracy: 0.5642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_34576\\3434567499.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./attach/best_model_checkpoint.pth'))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./attach/best_model_checkpoint.pth'))\n",
    "model.to(device)\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tar_to_index['<sos>'])\n",
    "print(tar_to_index['<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[동작시키기 (test 모드)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156   2   0   0]\n",
      "[  3 137   2   0   0   0   0   0   0   0]\n",
      "[137   2   4   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# seq2seq는 훈련 과정(교사 강요)과 테스트 과정에서의 동작 방식이 다름\n",
    "# 그래서 테스트 과정을 위해 모델을 다시 설계(디코더 수정)\n",
    "\n",
    "# 1) 번역하고자 하는 입력 문장이 인코더로 입력되어 인코더의 마지막 시점의 은닉 상태와 셀 상태를 얻는다\n",
    "# 2) 인코더의 은닉 상태와 셀 상태, 그리고 토큰 <sos>를 디코더로 전달\n",
    "# 3) 디코더가 토큰 <eos>가 나올 때까지 다음 단어를 예측하는 행동을 반복\n",
    "\n",
    "index_to_src = {i: w for w, i in src_to_index.items()}\n",
    "index_to_tar = {i: w for w, i in tar_to_index.items()}\n",
    "\n",
    "def seq_to_src(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0):\n",
    "            sentence = sentence + index_to_src[encoded_word]+' '\n",
    "    return sentence\n",
    "\n",
    "def seq_to_tar(input_seq):\n",
    "    sentence = ''\n",
    "    for encoded_word in input_seq:\n",
    "        if(encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']):\n",
    "            sentence = sentence + index_to_tar[encoded_word]+' '\n",
    "    return sentence\n",
    "\n",
    "print(encoder_input_test[25])\n",
    "print(decoder_input_test[25])\n",
    "print(decoder_target_test[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 단계에서는 디코더를 매 시점 별로 컨트롤\n",
    "# 사용될 변수는 decoder_input\n",
    "# unsquzzez(dim) : dim번째 차원에 차원추가\n",
    "\n",
    "def decode_sequence(input_seq, model, max_output_len, src_vocab_size, tar_vocab_size, int_to_src, int_to_tar):\n",
    "    # input_seq: 단어 1개의 특징벡터(embedding 벡터)\n",
    "    encoder_input = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device) # 3D (배치1)\n",
    "\n",
    "    # 인코더의 초기 상태 설정\n",
    "    hidden, cell = model.encoder(encoder_input)\n",
    "\n",
    "    # 시작 토큰 <SOS>를 디코더의 첫입력으로 설정\n",
    "    # unsquzzez(0)은 배치 차원 추가 목적\n",
    "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device) # 2D\n",
    "    decoded_tokens = []\n",
    "\n",
    "    # for문을 도는 것 == 디코더의 각 시점\n",
    "    # 디코더의 각 시점마다 이전 예측값을 decoder_input으로 전달\n",
    "    print(\"input_seq: \", input_seq)\n",
    "    for i in range(max_output_len):\n",
    "        output, hidden, cell = model.decoder(decoder_input, hidden, cell)\n",
    "        \n",
    "        # 소프트맥스 회귀. 예측 단어의 인덱스\n",
    "        output_token = output.argmax(dim=-1).item()\n",
    "        \n",
    "        # report\n",
    "        print(\"report=> {}th | decoder_input=> {} |output_token=> {}\".format(i,decoder_input, output_token))\n",
    "        \n",
    "        # 종료 토큰 <eos>\n",
    "        if output_token == 4:\n",
    "            print(\"decoded_tokens: \", decoded_tokens)\n",
    "            break\n",
    "\n",
    "        # 현재 시점의 예측값을 다음 시점의 입력으로 전달\n",
    "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        \n",
    "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴\n",
    "        decoded_tokens.append(output_token)\n",
    "\n",
    "\n",
    "    result = ' '.join(int_to_tar[token] for token in decoded_tokens)\n",
    "    print(\"result: \", result)\n",
    "    print(\"-\"*50) \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_index:  3\n",
      "input_seq:  [  4 221   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 6\n",
      "report=> 1th | decoder_input=> tensor([[6]]) |output_token=> 7\n",
      "report=> 2th | decoder_input=> tensor([[7]]) |output_token=> 2\n",
      "report=> 3th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [6, 7, 2]\n",
      "result:  je suis .\n",
      "--------------------------------------------------\n",
      "입력문장 : i pass . \n",
      "정답문장 : je passe . \n",
      "번역문장 : je suis .\n",
      "==================================================\n",
      "seq_index:  50\n",
      "input_seq:  [203   5   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 82\n",
      "report=> 1th | decoder_input=> tensor([[82]]) |output_token=> 8\n",
      "report=> 2th | decoder_input=> tensor([[8]]) |output_token=> 2\n",
      "report=> 3th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [82, 8, 2]\n",
      "result:  prenez le .\n",
      "--------------------------------------------------\n",
      "입력문장 : prove it . \n",
      "정답문장 : prouvez le . \n",
      "번역문장 : prenez le .\n",
      "==================================================\n",
      "seq_index:  70\n",
      "input_seq:  [125   2   0   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 199\n",
      "report=> 1th | decoder_input=> tensor([[199]]) |output_token=> 5\n",
      "report=> 2th | decoder_input=> tensor([[5]]) |output_token=> 4\n",
      "decoded_tokens:  [199, 5]\n",
      "result:  souriez !\n",
      "--------------------------------------------------\n",
      "입력문장 : smile . \n",
      "정답문장 : souriez ! \n",
      "번역문장 : souriez !\n",
      "==================================================\n",
      "seq_index:  99\n",
      "input_seq:  [ 30 149   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 9\n",
      "report=> 1th | decoder_input=> tensor([[9]]) |output_token=> 10\n",
      "report=> 2th | decoder_input=> tensor([[10]]) |output_token=> 2\n",
      "report=> 3th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [9, 10, 2]\n",
      "result:  j ai .\n",
      "--------------------------------------------------\n",
      "입력문장 : try some . \n",
      "정답문장 : essaies en ! \n",
      "번역문장 : j ai .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터에 대해 샘플 결과 확인\n",
    "\n",
    "for seq_index in [3,50,70,99]: \n",
    "    # -------- 데이터 단위 이해 --------------\n",
    "    # 리스트[]로 묶인 건 문장 단위이고, 리스트 안의 원소는 단어(vocab) 단위\n",
    "    #  ['go', '.'] 같은 문장을 단어로 쪼개어 인덱스화 해서 모은 것\n",
    "\n",
    "    # 여기서 [3,50,100,300]의 각 원소는 사실 문장단위로\n",
    "    # 번역 테스트할 문장을 리스트에 모아놓은 것\n",
    "    # 원소3을 encoder_input_train에서 인덱싱하면 [ 4 65  2  0] 문장!!\n",
    "\n",
    "    # encoder_input_train : 훈련에 사용된 문장들의 집합 (2D배열)\n",
    "    # input_seq: 위 문장집합에서 해당하는 문장 추출한 것 (1D배열)\n",
    "    print(\"seq_index: \", seq_index)\n",
    "    input_seq = encoder_input_train[seq_index] \n",
    "    translated_text = decode_sequence(input_seq, model, 20, src_vocab_size, tar_vocab_size, index_to_src, index_to_tar)\n",
    "\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",translated_text)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_index:  3\n",
      "input_seq:  [ 15 285   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 22\n",
      "report=> 1th | decoder_input=> tensor([[22]]) |output_token=> 5\n",
      "report=> 2th | decoder_input=> tensor([[5]]) |output_token=> 4\n",
      "decoded_tokens:  [22, 5]\n",
      "result:  soyez !\n",
      "--------------------------------------------------\n",
      "입력문장 : i pass . \n",
      "정답문장 : je passe . \n",
      "번역문장 : soyez !\n",
      "==================================================\n",
      "seq_index:  50\n",
      "input_seq:  [24  5  2  0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 74\n",
      "report=> 1th | decoder_input=> tensor([[74]]) |output_token=> 2\n",
      "report=> 2th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [74, 2]\n",
      "result:  deguerpissez .\n",
      "--------------------------------------------------\n",
      "입력문장 : prove it . \n",
      "정답문장 : prouvez le . \n",
      "번역문장 : deguerpissez .\n",
      "==================================================\n",
      "seq_index:  70\n",
      "input_seq:  [ 14 147   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 18\n",
      "report=> 1th | decoder_input=> tensor([[18]]) |output_token=> 12\n",
      "report=> 2th | decoder_input=> tensor([[12]]) |output_token=> 105\n",
      "report=> 3th | decoder_input=> tensor([[105]]) |output_token=> 2\n",
      "report=> 4th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [18, 12, 105, 2]\n",
      "result:  tom est parti .\n",
      "--------------------------------------------------\n",
      "입력문장 : smile . \n",
      "정답문장 : souriez ! \n",
      "번역문장 : tom est parti .\n",
      "==================================================\n",
      "seq_index:  99\n",
      "input_seq:  [117  25   2   0]\n",
      "report=> 0th | decoder_input=> tensor([[3]]) |output_token=> 20\n",
      "report=> 1th | decoder_input=> tensor([[20]]) |output_token=> 60\n",
      "report=> 2th | decoder_input=> tensor([[60]]) |output_token=> 2\n",
      "report=> 3th | decoder_input=> tensor([[2]]) |output_token=> 4\n",
      "decoded_tokens:  [20, 60, 2]\n",
      "result:  nous avons .\n",
      "--------------------------------------------------\n",
      "입력문장 : try some . \n",
      "정답문장 : essaies en ! \n",
      "번역문장 : nous avons .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 테스트트데이터에 대해 샘플 결과 확인\n",
    "\n",
    "for seq_index in [3,50,70,99]: \n",
    "    # -------- 데이터 단위 이해 --------------\n",
    "    # 리스트[]로 묶인 건 문장 단위이고, 리스트 안의 원소는 단어(vocab) 단위\n",
    "    #  ['go', '.'] 같은 문장을 단어로 쪼개어 인덱스화 해서 모은 것\n",
    "\n",
    "    # 여기서 [3,50,100,300]의 각 원소는 사실 문장단위로\n",
    "    # 번역 테스트할 문장을 리스트에 모아놓은 것\n",
    "    # 원소3을 encoder_input_train에서 인덱싱하면 [ 4 65  2  0] 문장!!\n",
    "\n",
    "    # encoder_input_train : 훈련에 사용된 문장들의 집합 (2D배열)\n",
    "    # input_seq: 위 문장집합에서 해당하는 문장 추출한 것 (1D배열)\n",
    "    print(\"seq_index: \", seq_index)\n",
    "    input_seq = encoder_input_test[seq_index] \n",
    "    translated_text = decode_sequence(input_seq, model, 20, src_vocab_size, tar_vocab_size, index_to_src, index_to_tar)\n",
    "\n",
    "\n",
    "    print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "    print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "    print(\"번역문장 :\",translated_text)\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH38_CPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
